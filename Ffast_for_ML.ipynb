{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ffast for ML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVts173loVXw/MxybCiBbl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterryjack/ffast/blob/main/Ffast_for_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FFast for NLP and ML:\n",
        "## Training an Intent classifier"
      ],
      "metadata": {
        "id": "FwLULmT-7ux-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Get some data"
      ],
      "metadata": {
        "id": "kB7b2LIf77DS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VRStgukRjrY",
        "outputId": "f44acff5-4a41-4320-f4e8-bdff20cd1a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 09:20:49--  https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2495390 (2.4M) [text/plain]\n",
            "Saving to: ‘data_full.json’\n",
            "\n",
            "data_full.json      100%[===================>]   2.38M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-03-17 09:20:49 (59.7 MB/s) - ‘data_full.json’ saved [2495390/2495390]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from json import loads\n",
        "\n",
        "with open('data_full.json') as json_file:\n",
        "  data = loads(json_file.read())"
      ],
      "metadata": {
        "id": "3ExHv1FZSD70"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data['oos_val'] + data['val']"
      ],
      "metadata": {
        "id": "Fn_UA1OaSRD8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences,target_intents = zip(*data_train)"
      ],
      "metadata": {
        "id": "PHIAaZ9m1kmN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7HlkDty9Tp7",
        "outputId": "ba8a1cd8-4eee-4b27-e6a3-88f146cbf0ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3100"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Encode Text data using FFast"
      ],
      "metadata": {
        "id": "Mzbfg2429yhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ffast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziHVJPJQTa-J",
        "outputId": "ea8f05ad-56c5-4c04-9f47-3f3ea8a2285a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffast in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.4.1)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (from ffast) (1.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.21.5)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from ffast) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ffast) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ffast) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ffast import load\n",
        "tokeniser = load(\"poincare\")"
      ],
      "metadata": {
        "id": "tgd7JdY-Tre8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_text = lambda text: tokeniser.encode(text).vector"
      ],
      "metadata": {
        "id": "PzEIzmB2S7Y2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentence_vectors = list(map(embed_text,sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itueHA-V-Ke0",
        "outputId": "f434c902-a3b9-461d-ee0d-6147abab42c3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INTENTS = sorted(set(target_intents))\n",
        "\n",
        "map_intent_id_to_name = lambda index: INTENTS[index]\n",
        "map_intent_name_to_id = lambda intent: INTENTS.index(intent)"
      ],
      "metadata": {
        "id": "OBKs86aq2U4G"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_intent_indexes = list(map(map_intent_name_to_id,target_intents))"
      ],
      "metadata": {
        "id": "rZFYNXb--TQx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Train an ML model"
      ],
      "metadata": {
        "id": "LJa4FAa7-bOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "5KpARS8iShda"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,100,100), \n",
        "    solver='sgd',\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=1000,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "Dmjys7F1SkaX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(train_sentence_vectors, train_intent_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krlq-Yqx-nyV",
        "outputId": "5aff1fb6-a20f-4aaa-e91c-21b13b3d672e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 5.21159110\n",
            "Iteration 2, loss = 5.04169411\n",
            "Iteration 3, loss = 5.01972198\n",
            "Iteration 4, loss = 5.00998344\n",
            "Iteration 5, loss = 5.00427517\n",
            "Iteration 6, loss = 4.99873623\n",
            "Iteration 7, loss = 4.99306475\n",
            "Iteration 8, loss = 4.98793056\n",
            "Iteration 9, loss = 4.98264149\n",
            "Iteration 10, loss = 4.97697353\n",
            "Iteration 11, loss = 4.97061008\n",
            "Iteration 12, loss = 4.96586751\n",
            "Iteration 13, loss = 4.96113108\n",
            "Iteration 14, loss = 4.95406509\n",
            "Iteration 15, loss = 4.94529742\n",
            "Iteration 16, loss = 4.93860574\n",
            "Iteration 17, loss = 4.92948993\n",
            "Iteration 18, loss = 4.92385151\n",
            "Iteration 19, loss = 4.91406792\n",
            "Iteration 20, loss = 4.90545090\n",
            "Iteration 21, loss = 4.89644922\n",
            "Iteration 22, loss = 4.88771513\n",
            "Iteration 23, loss = 4.87766571\n",
            "Iteration 24, loss = 4.86922112\n",
            "Iteration 25, loss = 4.85929422\n",
            "Iteration 26, loss = 4.84772293\n",
            "Iteration 27, loss = 4.83578531\n",
            "Iteration 28, loss = 4.82562266\n",
            "Iteration 29, loss = 4.81716725\n",
            "Iteration 30, loss = 4.80150351\n",
            "Iteration 31, loss = 4.79051471\n",
            "Iteration 32, loss = 4.77942368\n",
            "Iteration 33, loss = 4.76502566\n",
            "Iteration 34, loss = 4.75378563\n",
            "Iteration 35, loss = 4.73604958\n",
            "Iteration 36, loss = 4.72689970\n",
            "Iteration 37, loss = 4.70826284\n",
            "Iteration 38, loss = 4.69466805\n",
            "Iteration 39, loss = 4.68195002\n",
            "Iteration 40, loss = 4.66307869\n",
            "Iteration 41, loss = 4.64396771\n",
            "Iteration 42, loss = 4.62853594\n",
            "Iteration 43, loss = 4.60823238\n",
            "Iteration 44, loss = 4.59307257\n",
            "Iteration 45, loss = 4.57158670\n",
            "Iteration 46, loss = 4.55150603\n",
            "Iteration 47, loss = 4.53118366\n",
            "Iteration 48, loss = 4.50774514\n",
            "Iteration 49, loss = 4.48452931\n",
            "Iteration 50, loss = 4.46203418\n",
            "Iteration 51, loss = 4.43588323\n",
            "Iteration 52, loss = 4.41370439\n",
            "Iteration 53, loss = 4.38569915\n",
            "Iteration 54, loss = 4.36022824\n",
            "Iteration 55, loss = 4.32930992\n",
            "Iteration 56, loss = 4.30280699\n",
            "Iteration 57, loss = 4.27324568\n",
            "Iteration 58, loss = 4.24117816\n",
            "Iteration 59, loss = 4.21494150\n",
            "Iteration 60, loss = 4.17483510\n",
            "Iteration 61, loss = 4.14718931\n",
            "Iteration 62, loss = 4.11744138\n",
            "Iteration 63, loss = 4.08454880\n",
            "Iteration 64, loss = 4.05117437\n",
            "Iteration 65, loss = 4.01826992\n",
            "Iteration 66, loss = 3.97988038\n",
            "Iteration 67, loss = 3.94673973\n",
            "Iteration 68, loss = 3.91920610\n",
            "Iteration 69, loss = 3.87546817\n",
            "Iteration 70, loss = 3.84602913\n",
            "Iteration 71, loss = 3.81338696\n",
            "Iteration 72, loss = 3.78181598\n",
            "Iteration 73, loss = 3.75168001\n",
            "Iteration 74, loss = 3.72104911\n",
            "Iteration 75, loss = 3.68265091\n",
            "Iteration 76, loss = 3.65533535\n",
            "Iteration 77, loss = 3.62797333\n",
            "Iteration 78, loss = 3.59289215\n",
            "Iteration 79, loss = 3.55546725\n",
            "Iteration 80, loss = 3.52686463\n",
            "Iteration 81, loss = 3.49477718\n",
            "Iteration 82, loss = 3.45983251\n",
            "Iteration 83, loss = 3.43680729\n",
            "Iteration 84, loss = 3.39912020\n",
            "Iteration 85, loss = 3.36897423\n",
            "Iteration 86, loss = 3.34022266\n",
            "Iteration 87, loss = 3.30813040\n",
            "Iteration 88, loss = 3.28345440\n",
            "Iteration 89, loss = 3.25217928\n",
            "Iteration 90, loss = 3.22629215\n",
            "Iteration 91, loss = 3.19592521\n",
            "Iteration 92, loss = 3.16460195\n",
            "Iteration 93, loss = 3.14109400\n",
            "Iteration 94, loss = 3.10923301\n",
            "Iteration 95, loss = 3.09446671\n",
            "Iteration 96, loss = 3.05816112\n",
            "Iteration 97, loss = 3.03635960\n",
            "Iteration 98, loss = 2.99496210\n",
            "Iteration 99, loss = 2.98034111\n",
            "Iteration 100, loss = 2.96054934\n",
            "Iteration 101, loss = 2.93131628\n",
            "Iteration 102, loss = 2.90029956\n",
            "Iteration 103, loss = 2.87576448\n",
            "Iteration 104, loss = 2.85345824\n",
            "Iteration 105, loss = 2.81789427\n",
            "Iteration 106, loss = 2.79864158\n",
            "Iteration 107, loss = 2.77346642\n",
            "Iteration 108, loss = 2.75530670\n",
            "Iteration 109, loss = 2.73363536\n",
            "Iteration 110, loss = 2.69709003\n",
            "Iteration 111, loss = 2.68167333\n",
            "Iteration 112, loss = 2.66167569\n",
            "Iteration 113, loss = 2.64359527\n",
            "Iteration 114, loss = 2.61968757\n",
            "Iteration 115, loss = 2.58763512\n",
            "Iteration 116, loss = 2.56889038\n",
            "Iteration 117, loss = 2.54777702\n",
            "Iteration 118, loss = 2.53058438\n",
            "Iteration 119, loss = 2.50800155\n",
            "Iteration 120, loss = 2.49597371\n",
            "Iteration 121, loss = 2.46106505\n",
            "Iteration 122, loss = 2.44259360\n",
            "Iteration 123, loss = 2.43044924\n",
            "Iteration 124, loss = 2.40756164\n",
            "Iteration 125, loss = 2.38600109\n",
            "Iteration 126, loss = 2.37328111\n",
            "Iteration 127, loss = 2.34566790\n",
            "Iteration 128, loss = 2.33243015\n",
            "Iteration 129, loss = 2.32489224\n",
            "Iteration 130, loss = 2.29248501\n",
            "Iteration 131, loss = 2.27866446\n",
            "Iteration 132, loss = 2.26750280\n",
            "Iteration 133, loss = 2.23918518\n",
            "Iteration 134, loss = 2.21962307\n",
            "Iteration 135, loss = 2.20290461\n",
            "Iteration 136, loss = 2.19073578\n",
            "Iteration 137, loss = 2.17802651\n",
            "Iteration 138, loss = 2.16312308\n",
            "Iteration 139, loss = 2.14497223\n",
            "Iteration 140, loss = 2.12846236\n",
            "Iteration 141, loss = 2.11298396\n",
            "Iteration 142, loss = 2.09815254\n",
            "Iteration 143, loss = 2.08048547\n",
            "Iteration 144, loss = 2.06365776\n",
            "Iteration 145, loss = 2.04505513\n",
            "Iteration 146, loss = 2.03150815\n",
            "Iteration 147, loss = 2.00935133\n",
            "Iteration 148, loss = 2.00089570\n",
            "Iteration 149, loss = 1.98411258\n",
            "Iteration 150, loss = 1.96666671\n",
            "Iteration 151, loss = 1.95188220\n",
            "Iteration 152, loss = 1.93670006\n",
            "Iteration 153, loss = 1.91922176\n",
            "Iteration 154, loss = 1.91921547\n",
            "Iteration 155, loss = 1.90959725\n",
            "Iteration 156, loss = 1.88925035\n",
            "Iteration 157, loss = 1.87227375\n",
            "Iteration 158, loss = 1.85708957\n",
            "Iteration 159, loss = 1.84701698\n",
            "Iteration 160, loss = 1.83368089\n",
            "Iteration 161, loss = 1.80653326\n",
            "Iteration 162, loss = 1.80540338\n",
            "Iteration 163, loss = 1.80068839\n",
            "Iteration 164, loss = 1.76821771\n",
            "Iteration 165, loss = 1.76922607\n",
            "Iteration 166, loss = 1.75451138\n",
            "Iteration 167, loss = 1.73889055\n",
            "Iteration 168, loss = 1.73045035\n",
            "Iteration 169, loss = 1.71233001\n",
            "Iteration 170, loss = 1.71070275\n",
            "Iteration 171, loss = 1.69945393\n",
            "Iteration 172, loss = 1.67395401\n",
            "Iteration 173, loss = 1.67272491\n",
            "Iteration 174, loss = 1.66731967\n",
            "Iteration 175, loss = 1.64457596\n",
            "Iteration 176, loss = 1.62335283\n",
            "Iteration 177, loss = 1.62711211\n",
            "Iteration 178, loss = 1.61572084\n",
            "Iteration 179, loss = 1.60999333\n",
            "Iteration 180, loss = 1.58327811\n",
            "Iteration 181, loss = 1.57438566\n",
            "Iteration 182, loss = 1.56644788\n",
            "Iteration 183, loss = 1.56745224\n",
            "Iteration 184, loss = 1.52747120\n",
            "Iteration 185, loss = 1.55180327\n",
            "Iteration 186, loss = 1.52385388\n",
            "Iteration 187, loss = 1.50935840\n",
            "Iteration 188, loss = 1.50835478\n",
            "Iteration 189, loss = 1.49540721\n",
            "Iteration 190, loss = 1.48683297\n",
            "Iteration 191, loss = 1.47052122\n",
            "Iteration 192, loss = 1.46160964\n",
            "Iteration 193, loss = 1.45037077\n",
            "Iteration 194, loss = 1.45545519\n",
            "Iteration 195, loss = 1.42969543\n",
            "Iteration 196, loss = 1.42602599\n",
            "Iteration 197, loss = 1.41501534\n",
            "Iteration 198, loss = 1.40168563\n",
            "Iteration 199, loss = 1.38644282\n",
            "Iteration 200, loss = 1.39604814\n",
            "Iteration 201, loss = 1.38252461\n",
            "Iteration 202, loss = 1.37661396\n",
            "Iteration 203, loss = 1.38203770\n",
            "Iteration 204, loss = 1.34446512\n",
            "Iteration 205, loss = 1.35030592\n",
            "Iteration 206, loss = 1.34970144\n",
            "Iteration 207, loss = 1.31648505\n",
            "Iteration 208, loss = 1.33008569\n",
            "Iteration 209, loss = 1.29838198\n",
            "Iteration 210, loss = 1.29286125\n",
            "Iteration 211, loss = 1.31319064\n",
            "Iteration 212, loss = 1.29080594\n",
            "Iteration 213, loss = 1.26198956\n",
            "Iteration 214, loss = 1.26647502\n",
            "Iteration 215, loss = 1.24075952\n",
            "Iteration 216, loss = 1.25242390\n",
            "Iteration 217, loss = 1.23026747\n",
            "Iteration 218, loss = 1.23122297\n",
            "Iteration 219, loss = 1.20760937\n",
            "Iteration 220, loss = 1.22865215\n",
            "Iteration 221, loss = 1.21633043\n",
            "Iteration 222, loss = 1.21619584\n",
            "Iteration 223, loss = 1.18306866\n",
            "Iteration 224, loss = 1.18393759\n",
            "Iteration 225, loss = 1.17205125\n",
            "Iteration 226, loss = 1.16441835\n",
            "Iteration 227, loss = 1.13871548\n",
            "Iteration 228, loss = 1.14582228\n",
            "Iteration 229, loss = 1.15012381\n",
            "Iteration 230, loss = 1.12407647\n",
            "Iteration 231, loss = 1.14720065\n",
            "Iteration 232, loss = 1.13560538\n",
            "Iteration 233, loss = 1.12708193\n",
            "Iteration 234, loss = 1.09915396\n",
            "Iteration 235, loss = 1.10565678\n",
            "Iteration 236, loss = 1.08752290\n",
            "Iteration 237, loss = 1.08062344\n",
            "Iteration 238, loss = 1.06911370\n",
            "Iteration 239, loss = 1.08147266\n",
            "Iteration 240, loss = 1.06677602\n",
            "Iteration 241, loss = 1.06266049\n",
            "Iteration 242, loss = 1.04958469\n",
            "Iteration 243, loss = 1.03811971\n",
            "Iteration 244, loss = 1.04088444\n",
            "Iteration 245, loss = 1.03064812\n",
            "Iteration 246, loss = 1.03348047\n",
            "Iteration 247, loss = 1.02181529\n",
            "Iteration 248, loss = 0.99932177\n",
            "Iteration 249, loss = 1.00233362\n",
            "Iteration 250, loss = 0.99494525\n",
            "Iteration 251, loss = 1.00767996\n",
            "Iteration 252, loss = 0.97563579\n",
            "Iteration 253, loss = 0.97829056\n",
            "Iteration 254, loss = 0.97763535\n",
            "Iteration 255, loss = 0.94252565\n",
            "Iteration 256, loss = 0.94895346\n",
            "Iteration 257, loss = 0.95591376\n",
            "Iteration 258, loss = 0.93481155\n",
            "Iteration 259, loss = 0.92934200\n",
            "Iteration 260, loss = 0.93675636\n",
            "Iteration 261, loss = 0.92318568\n",
            "Iteration 262, loss = 0.90317007\n",
            "Iteration 263, loss = 0.91531803\n",
            "Iteration 264, loss = 0.89872526\n",
            "Iteration 265, loss = 0.90003845\n",
            "Iteration 266, loss = 0.88580314\n",
            "Iteration 267, loss = 0.88526546\n",
            "Iteration 268, loss = 0.88720035\n",
            "Iteration 269, loss = 0.87880051\n",
            "Iteration 270, loss = 0.85908169\n",
            "Iteration 271, loss = 0.85772839\n",
            "Iteration 272, loss = 0.84723576\n",
            "Iteration 273, loss = 0.84664126\n",
            "Iteration 274, loss = 0.85527439\n",
            "Iteration 275, loss = 0.83398244\n",
            "Iteration 276, loss = 0.86064683\n",
            "Iteration 277, loss = 0.83077121\n",
            "Iteration 278, loss = 0.82171366\n",
            "Iteration 279, loss = 0.81934810\n",
            "Iteration 280, loss = 0.79600888\n",
            "Iteration 281, loss = 0.80287211\n",
            "Iteration 282, loss = 0.80902865\n",
            "Iteration 283, loss = 0.79107456\n",
            "Iteration 284, loss = 0.77079319\n",
            "Iteration 285, loss = 0.78256649\n",
            "Iteration 286, loss = 0.78021102\n",
            "Iteration 287, loss = 0.76407477\n",
            "Iteration 288, loss = 0.76454715\n",
            "Iteration 289, loss = 0.73986933\n",
            "Iteration 290, loss = 0.75357074\n",
            "Iteration 291, loss = 0.74654653\n",
            "Iteration 292, loss = 0.73090297\n",
            "Iteration 293, loss = 0.75709125\n",
            "Iteration 294, loss = 0.73887019\n",
            "Iteration 295, loss = 0.71782023\n",
            "Iteration 296, loss = 0.72917034\n",
            "Iteration 297, loss = 0.72674870\n",
            "Iteration 298, loss = 0.70962564\n",
            "Iteration 299, loss = 0.70759756\n",
            "Iteration 300, loss = 0.70637347\n",
            "Iteration 301, loss = 0.69685511\n",
            "Iteration 302, loss = 0.69245632\n",
            "Iteration 303, loss = 0.67363008\n",
            "Iteration 304, loss = 0.68103958\n",
            "Iteration 305, loss = 0.65930893\n",
            "Iteration 306, loss = 0.67217552\n",
            "Iteration 307, loss = 0.66388996\n",
            "Iteration 308, loss = 0.67741874\n",
            "Iteration 309, loss = 0.66120701\n",
            "Iteration 310, loss = 0.65392330\n",
            "Iteration 311, loss = 0.63099852\n",
            "Iteration 312, loss = 0.63678657\n",
            "Iteration 313, loss = 0.65461932\n",
            "Iteration 314, loss = 0.64569419\n",
            "Iteration 315, loss = 0.62540006\n",
            "Iteration 316, loss = 0.61571151\n",
            "Iteration 317, loss = 0.62946941\n",
            "Iteration 318, loss = 0.60792885\n",
            "Iteration 319, loss = 0.60146989\n",
            "Iteration 320, loss = 0.59963857\n",
            "Iteration 321, loss = 0.60204690\n",
            "Iteration 322, loss = 0.58169207\n",
            "Iteration 323, loss = 0.57936777\n",
            "Iteration 324, loss = 0.59749406\n",
            "Iteration 325, loss = 0.59067114\n",
            "Iteration 326, loss = 0.57741769\n",
            "Iteration 327, loss = 0.56000900\n",
            "Iteration 328, loss = 0.57329798\n",
            "Iteration 329, loss = 0.55079021\n",
            "Iteration 330, loss = 0.56238999\n",
            "Iteration 331, loss = 0.54806906\n",
            "Iteration 332, loss = 0.54477857\n",
            "Iteration 333, loss = 0.55769749\n",
            "Iteration 334, loss = 0.54059376\n",
            "Iteration 335, loss = 0.54291924\n",
            "Iteration 336, loss = 0.53826106\n",
            "Iteration 337, loss = 0.52165846\n",
            "Iteration 338, loss = 0.52223959\n",
            "Iteration 339, loss = 0.51734143\n",
            "Iteration 340, loss = 0.51875300\n",
            "Iteration 341, loss = 0.50524039\n",
            "Iteration 342, loss = 0.49751314\n",
            "Iteration 343, loss = 0.51581167\n",
            "Iteration 344, loss = 0.49285471\n",
            "Iteration 345, loss = 0.48843166\n",
            "Iteration 346, loss = 0.50686723\n",
            "Iteration 347, loss = 0.48998483\n",
            "Iteration 348, loss = 0.48921390\n",
            "Iteration 349, loss = 0.47220358\n",
            "Iteration 350, loss = 0.47809386\n",
            "Iteration 351, loss = 0.46570454\n",
            "Iteration 352, loss = 0.47444653\n",
            "Iteration 353, loss = 0.45199558\n",
            "Iteration 354, loss = 0.44805198\n",
            "Iteration 355, loss = 0.45672369\n",
            "Iteration 356, loss = 0.45088724\n",
            "Iteration 357, loss = 0.44165916\n",
            "Iteration 358, loss = 0.44426108\n",
            "Iteration 359, loss = 0.43757598\n",
            "Iteration 360, loss = 0.43177505\n",
            "Iteration 361, loss = 0.44087304\n",
            "Iteration 362, loss = 0.44402648\n",
            "Iteration 363, loss = 0.43274865\n",
            "Iteration 364, loss = 0.43343656\n",
            "Iteration 365, loss = 0.42249662\n",
            "Iteration 366, loss = 0.44261158\n",
            "Iteration 367, loss = 0.41561065\n",
            "Iteration 368, loss = 0.41905789\n",
            "Iteration 369, loss = 0.41615079\n",
            "Iteration 370, loss = 0.40619181\n",
            "Iteration 371, loss = 0.39520893\n",
            "Iteration 372, loss = 0.38907408\n",
            "Iteration 373, loss = 0.38931377\n",
            "Iteration 374, loss = 0.39151306\n",
            "Iteration 375, loss = 0.39008008\n",
            "Iteration 376, loss = 0.37314740\n",
            "Iteration 377, loss = 0.37675760\n",
            "Iteration 378, loss = 0.37073078\n",
            "Iteration 379, loss = 0.36739388\n",
            "Iteration 380, loss = 0.37041242\n",
            "Iteration 381, loss = 0.36869531\n",
            "Iteration 382, loss = 0.37863257\n",
            "Iteration 383, loss = 0.35708351\n",
            "Iteration 384, loss = 0.36181419\n",
            "Iteration 385, loss = 0.36483868\n",
            "Iteration 386, loss = 0.34842210\n",
            "Iteration 387, loss = 0.35003988\n",
            "Iteration 388, loss = 0.34441575\n",
            "Iteration 389, loss = 0.35353972\n",
            "Iteration 390, loss = 0.33994517\n",
            "Iteration 391, loss = 0.34044762\n",
            "Iteration 392, loss = 0.33900267\n",
            "Iteration 393, loss = 0.32428374\n",
            "Iteration 394, loss = 0.33288273\n",
            "Iteration 395, loss = 0.33262440\n",
            "Iteration 396, loss = 0.31516535\n",
            "Iteration 397, loss = 0.31140361\n",
            "Iteration 398, loss = 0.32108042\n",
            "Iteration 399, loss = 0.30472613\n",
            "Iteration 400, loss = 0.31385382\n",
            "Iteration 401, loss = 0.30443688\n",
            "Iteration 402, loss = 0.30533076\n",
            "Iteration 403, loss = 0.30265207\n",
            "Iteration 404, loss = 0.30547308\n",
            "Iteration 405, loss = 0.29472597\n",
            "Iteration 406, loss = 0.29760582\n",
            "Iteration 407, loss = 0.28885287\n",
            "Iteration 408, loss = 0.28985856\n",
            "Iteration 409, loss = 0.28713445\n",
            "Iteration 410, loss = 0.28193031\n",
            "Iteration 411, loss = 0.28609824\n",
            "Iteration 412, loss = 0.27623142\n",
            "Iteration 413, loss = 0.27872019\n",
            "Iteration 414, loss = 0.27291880\n",
            "Iteration 415, loss = 0.26832875\n",
            "Iteration 416, loss = 0.26655278\n",
            "Iteration 417, loss = 0.27171838\n",
            "Iteration 418, loss = 0.26487465\n",
            "Iteration 419, loss = 0.26076658\n",
            "Iteration 420, loss = 0.26235641\n",
            "Iteration 421, loss = 0.25832383\n",
            "Iteration 422, loss = 0.25801850\n",
            "Iteration 423, loss = 0.25534891\n",
            "Iteration 424, loss = 0.24326566\n",
            "Iteration 425, loss = 0.24525977\n",
            "Iteration 426, loss = 0.24105282\n",
            "Iteration 427, loss = 0.24893958\n",
            "Iteration 428, loss = 0.23900249\n",
            "Iteration 429, loss = 0.23887781\n",
            "Iteration 430, loss = 0.23063858\n",
            "Iteration 431, loss = 0.22920472\n",
            "Iteration 432, loss = 0.23774041\n",
            "Iteration 433, loss = 0.22499353\n",
            "Iteration 434, loss = 0.22622506\n",
            "Iteration 435, loss = 0.22906311\n",
            "Iteration 436, loss = 0.24737667\n",
            "Iteration 437, loss = 0.21907164\n",
            "Iteration 438, loss = 0.22149153\n",
            "Iteration 439, loss = 0.21662807\n",
            "Iteration 440, loss = 0.21284625\n",
            "Iteration 441, loss = 0.20745581\n",
            "Iteration 442, loss = 0.21027688\n",
            "Iteration 443, loss = 0.20636236\n",
            "Iteration 444, loss = 0.20911225\n",
            "Iteration 445, loss = 0.20288622\n",
            "Iteration 446, loss = 0.19616548\n",
            "Iteration 447, loss = 0.20194250\n",
            "Iteration 448, loss = 0.20126338\n",
            "Iteration 449, loss = 0.19427676\n",
            "Iteration 450, loss = 0.19678763\n",
            "Iteration 451, loss = 0.19022112\n",
            "Iteration 452, loss = 0.19188783\n",
            "Iteration 453, loss = 0.19095963\n",
            "Iteration 454, loss = 0.19151504\n",
            "Iteration 455, loss = 0.18474799\n",
            "Iteration 456, loss = 0.18540352\n",
            "Iteration 457, loss = 0.18075593\n",
            "Iteration 458, loss = 0.17662729\n",
            "Iteration 459, loss = 0.17378984\n",
            "Iteration 460, loss = 0.17276031\n",
            "Iteration 461, loss = 0.17355059\n",
            "Iteration 462, loss = 0.17445178\n",
            "Iteration 463, loss = 0.17250685\n",
            "Iteration 464, loss = 0.16630418\n",
            "Iteration 465, loss = 0.16669365\n",
            "Iteration 466, loss = 0.16747302\n",
            "Iteration 467, loss = 0.16376184\n",
            "Iteration 468, loss = 0.16539312\n",
            "Iteration 469, loss = 0.16434737\n",
            "Iteration 470, loss = 0.16112010\n",
            "Iteration 471, loss = 0.15840115\n",
            "Iteration 472, loss = 0.15766194\n",
            "Iteration 473, loss = 0.15757933\n",
            "Iteration 474, loss = 0.15235549\n",
            "Iteration 475, loss = 0.15548921\n",
            "Iteration 476, loss = 0.15141714\n",
            "Iteration 477, loss = 0.15111228\n",
            "Iteration 478, loss = 0.15121535\n",
            "Iteration 479, loss = 0.14377865\n",
            "Iteration 480, loss = 0.14783128\n",
            "Iteration 481, loss = 0.14573379\n",
            "Iteration 482, loss = 0.14826349\n",
            "Iteration 483, loss = 0.14260817\n",
            "Iteration 484, loss = 0.14036299\n",
            "Iteration 485, loss = 0.13931097\n",
            "Iteration 486, loss = 0.13813606\n",
            "Iteration 487, loss = 0.13541745\n",
            "Iteration 488, loss = 0.13572699\n",
            "Iteration 489, loss = 0.13078004\n",
            "Iteration 490, loss = 0.13132281\n",
            "Iteration 491, loss = 0.13079140\n",
            "Iteration 492, loss = 0.13267185\n",
            "Iteration 493, loss = 0.12880165\n",
            "Iteration 494, loss = 0.12900042\n",
            "Iteration 495, loss = 0.12572975\n",
            "Iteration 496, loss = 0.12736780\n",
            "Iteration 497, loss = 0.12487667\n",
            "Iteration 498, loss = 0.12274006\n",
            "Iteration 499, loss = 0.12093874\n",
            "Iteration 500, loss = 0.12336688\n",
            "Iteration 501, loss = 0.12089288\n",
            "Iteration 502, loss = 0.12010062\n",
            "Iteration 503, loss = 0.11753364\n",
            "Iteration 504, loss = 0.11950891\n",
            "Iteration 505, loss = 0.11858872\n",
            "Iteration 506, loss = 0.11724173\n",
            "Iteration 507, loss = 0.11111189\n",
            "Iteration 508, loss = 0.11451681\n",
            "Iteration 509, loss = 0.11033737\n",
            "Iteration 510, loss = 0.11045181\n",
            "Iteration 511, loss = 0.10943090\n",
            "Iteration 512, loss = 0.10773892\n",
            "Iteration 513, loss = 0.10831689\n",
            "Iteration 514, loss = 0.11038750\n",
            "Iteration 515, loss = 0.10855913\n",
            "Iteration 516, loss = 0.10830852\n",
            "Iteration 517, loss = 0.10559529\n",
            "Iteration 518, loss = 0.10538288\n",
            "Iteration 519, loss = 0.10267956\n",
            "Iteration 520, loss = 0.10196289\n",
            "Iteration 521, loss = 0.10205834\n",
            "Iteration 522, loss = 0.10214376\n",
            "Iteration 523, loss = 0.09957257\n",
            "Iteration 524, loss = 0.09872023\n",
            "Iteration 525, loss = 0.09745419\n",
            "Iteration 526, loss = 0.09688522\n",
            "Iteration 527, loss = 0.09578988\n",
            "Iteration 528, loss = 0.09595839\n",
            "Iteration 529, loss = 0.09529613\n",
            "Iteration 530, loss = 0.09402434\n",
            "Iteration 531, loss = 0.09260073\n",
            "Iteration 532, loss = 0.09033106\n",
            "Iteration 533, loss = 0.09071892\n",
            "Iteration 534, loss = 0.09080278\n",
            "Iteration 535, loss = 0.08899253\n",
            "Iteration 536, loss = 0.09029467\n",
            "Iteration 537, loss = 0.08883876\n",
            "Iteration 538, loss = 0.08764105\n",
            "Iteration 539, loss = 0.08669200\n",
            "Iteration 540, loss = 0.08553287\n",
            "Iteration 541, loss = 0.08633412\n",
            "Iteration 542, loss = 0.08474521\n",
            "Iteration 543, loss = 0.08441769\n",
            "Iteration 544, loss = 0.08405785\n",
            "Iteration 545, loss = 0.08344543\n",
            "Iteration 546, loss = 0.08293400\n",
            "Iteration 547, loss = 0.08147691\n",
            "Iteration 548, loss = 0.08232750\n",
            "Iteration 549, loss = 0.08289875\n",
            "Iteration 550, loss = 0.08110382\n",
            "Iteration 551, loss = 0.07935854\n",
            "Iteration 552, loss = 0.07939909\n",
            "Iteration 553, loss = 0.07868890\n",
            "Iteration 554, loss = 0.07788813\n",
            "Iteration 555, loss = 0.07760683\n",
            "Iteration 556, loss = 0.07660374\n",
            "Iteration 557, loss = 0.07576368\n",
            "Iteration 558, loss = 0.07473084\n",
            "Iteration 559, loss = 0.07447427\n",
            "Iteration 560, loss = 0.07471548\n",
            "Iteration 561, loss = 0.07379809\n",
            "Iteration 562, loss = 0.07330001\n",
            "Iteration 563, loss = 0.07262521\n",
            "Iteration 564, loss = 0.07199948\n",
            "Iteration 565, loss = 0.07189533\n",
            "Iteration 566, loss = 0.07326872\n",
            "Iteration 567, loss = 0.07151169\n",
            "Iteration 568, loss = 0.07296909\n",
            "Iteration 569, loss = 0.07071569\n",
            "Iteration 570, loss = 0.07035918\n",
            "Iteration 571, loss = 0.06893313\n",
            "Iteration 572, loss = 0.06814953\n",
            "Iteration 573, loss = 0.06797576\n",
            "Iteration 574, loss = 0.06700769\n",
            "Iteration 575, loss = 0.06751173\n",
            "Iteration 576, loss = 0.06675495\n",
            "Iteration 577, loss = 0.06591742\n",
            "Iteration 578, loss = 0.06608567\n",
            "Iteration 579, loss = 0.06511933\n",
            "Iteration 580, loss = 0.06424871\n",
            "Iteration 581, loss = 0.06549019\n",
            "Iteration 582, loss = 0.06427735\n",
            "Iteration 583, loss = 0.06339818\n",
            "Iteration 584, loss = 0.06311664\n",
            "Iteration 585, loss = 0.06303344\n",
            "Iteration 586, loss = 0.06267210\n",
            "Iteration 587, loss = 0.06165903\n",
            "Iteration 588, loss = 0.06185986\n",
            "Iteration 589, loss = 0.06147783\n",
            "Iteration 590, loss = 0.06064386\n",
            "Iteration 591, loss = 0.06048871\n",
            "Iteration 592, loss = 0.06030306\n",
            "Iteration 593, loss = 0.05974749\n",
            "Iteration 594, loss = 0.05944730\n",
            "Iteration 595, loss = 0.05884446\n",
            "Iteration 596, loss = 0.05877821\n",
            "Iteration 597, loss = 0.05851176\n",
            "Iteration 598, loss = 0.05786313\n",
            "Iteration 599, loss = 0.05793148\n",
            "Iteration 600, loss = 0.05730114\n",
            "Iteration 601, loss = 0.05690840\n",
            "Iteration 602, loss = 0.05642621\n",
            "Iteration 603, loss = 0.05606164\n",
            "Iteration 604, loss = 0.05537679\n",
            "Iteration 605, loss = 0.05543626\n",
            "Iteration 606, loss = 0.05547249\n",
            "Iteration 607, loss = 0.05446197\n",
            "Iteration 608, loss = 0.05419563\n",
            "Iteration 609, loss = 0.05357005\n",
            "Iteration 610, loss = 0.05360926\n",
            "Iteration 611, loss = 0.05287566\n",
            "Iteration 612, loss = 0.05259740\n",
            "Iteration 613, loss = 0.05304748\n",
            "Iteration 614, loss = 0.05251727\n",
            "Iteration 615, loss = 0.05231495\n",
            "Iteration 616, loss = 0.05173891\n",
            "Iteration 617, loss = 0.05145024\n",
            "Iteration 618, loss = 0.05092846\n",
            "Iteration 619, loss = 0.05117589\n",
            "Iteration 620, loss = 0.05049027\n",
            "Iteration 621, loss = 0.05060377\n",
            "Iteration 622, loss = 0.04987879\n",
            "Iteration 623, loss = 0.05062583\n",
            "Iteration 624, loss = 0.05032465\n",
            "Iteration 625, loss = 0.04979230\n",
            "Iteration 626, loss = 0.04965156\n",
            "Iteration 627, loss = 0.04877626\n",
            "Iteration 628, loss = 0.04832531\n",
            "Iteration 629, loss = 0.04823155\n",
            "Iteration 630, loss = 0.04836781\n",
            "Iteration 631, loss = 0.04735021\n",
            "Iteration 632, loss = 0.04784293\n",
            "Iteration 633, loss = 0.04713873\n",
            "Iteration 634, loss = 0.04732208\n",
            "Iteration 635, loss = 0.04704103\n",
            "Iteration 636, loss = 0.04637177\n",
            "Iteration 637, loss = 0.04578023\n",
            "Iteration 638, loss = 0.04599782\n",
            "Iteration 639, loss = 0.04555476\n",
            "Iteration 640, loss = 0.04566445\n",
            "Iteration 641, loss = 0.04513790\n",
            "Iteration 642, loss = 0.04538910\n",
            "Iteration 643, loss = 0.04544381\n",
            "Iteration 644, loss = 0.04439112\n",
            "Iteration 645, loss = 0.04447993\n",
            "Iteration 646, loss = 0.04389303\n",
            "Iteration 647, loss = 0.04337497\n",
            "Iteration 648, loss = 0.04313529\n",
            "Iteration 649, loss = 0.04335151\n",
            "Iteration 650, loss = 0.04395290\n",
            "Iteration 651, loss = 0.04317602\n",
            "Iteration 652, loss = 0.04294700\n",
            "Iteration 653, loss = 0.04248252\n",
            "Iteration 654, loss = 0.04249005\n",
            "Iteration 655, loss = 0.04189973\n",
            "Iteration 656, loss = 0.04232618\n",
            "Iteration 657, loss = 0.04146749\n",
            "Iteration 658, loss = 0.04154493\n",
            "Iteration 659, loss = 0.04113400\n",
            "Iteration 660, loss = 0.04121023\n",
            "Iteration 661, loss = 0.04079173\n",
            "Iteration 662, loss = 0.04090564\n",
            "Iteration 663, loss = 0.04099967\n",
            "Iteration 664, loss = 0.04038237\n",
            "Iteration 665, loss = 0.04002395\n",
            "Iteration 666, loss = 0.04035597\n",
            "Iteration 667, loss = 0.03953904\n",
            "Iteration 668, loss = 0.03962831\n",
            "Iteration 669, loss = 0.03985800\n",
            "Iteration 670, loss = 0.03930177\n",
            "Iteration 671, loss = 0.03905366\n",
            "Iteration 672, loss = 0.03924375\n",
            "Iteration 673, loss = 0.03874626\n",
            "Iteration 674, loss = 0.03865264\n",
            "Iteration 675, loss = 0.03812791\n",
            "Iteration 676, loss = 0.03823048\n",
            "Iteration 677, loss = 0.03792520\n",
            "Iteration 678, loss = 0.03810116\n",
            "Iteration 679, loss = 0.03789296\n",
            "Iteration 680, loss = 0.03747542\n",
            "Iteration 681, loss = 0.03757220\n",
            "Iteration 682, loss = 0.03766952\n",
            "Iteration 683, loss = 0.03763270\n",
            "Iteration 684, loss = 0.03672340\n",
            "Iteration 685, loss = 0.03703171\n",
            "Iteration 686, loss = 0.03669941\n",
            "Iteration 687, loss = 0.03620598\n",
            "Iteration 688, loss = 0.03602903\n",
            "Iteration 689, loss = 0.03604320\n",
            "Iteration 690, loss = 0.03608867\n",
            "Iteration 691, loss = 0.03584413\n",
            "Iteration 692, loss = 0.03591147\n",
            "Iteration 693, loss = 0.03533799\n",
            "Iteration 694, loss = 0.03524446\n",
            "Iteration 695, loss = 0.03490198\n",
            "Iteration 696, loss = 0.03510161\n",
            "Iteration 697, loss = 0.03519970\n",
            "Iteration 698, loss = 0.03451996\n",
            "Iteration 699, loss = 0.03469395\n",
            "Iteration 700, loss = 0.03453935\n",
            "Iteration 701, loss = 0.03405787\n",
            "Iteration 702, loss = 0.03421386\n",
            "Iteration 703, loss = 0.03383333\n",
            "Iteration 704, loss = 0.03347627\n",
            "Iteration 705, loss = 0.03388140\n",
            "Iteration 706, loss = 0.03376285\n",
            "Iteration 707, loss = 0.03352164\n",
            "Iteration 708, loss = 0.03343648\n",
            "Iteration 709, loss = 0.03304765\n",
            "Iteration 710, loss = 0.03293077\n",
            "Iteration 711, loss = 0.03287199\n",
            "Iteration 712, loss = 0.03292457\n",
            "Iteration 713, loss = 0.03293229\n",
            "Iteration 714, loss = 0.03256505\n",
            "Iteration 715, loss = 0.03232661\n",
            "Iteration 716, loss = 0.03234688\n",
            "Iteration 717, loss = 0.03250307\n",
            "Iteration 718, loss = 0.03189333\n",
            "Iteration 719, loss = 0.03207282\n",
            "Iteration 720, loss = 0.03180276\n",
            "Iteration 721, loss = 0.03150296\n",
            "Iteration 722, loss = 0.03155910\n",
            "Iteration 723, loss = 0.03144246\n",
            "Iteration 724, loss = 0.03143131\n",
            "Iteration 725, loss = 0.03120194\n",
            "Iteration 726, loss = 0.03115418\n",
            "Iteration 727, loss = 0.03113245\n",
            "Iteration 728, loss = 0.03077100\n",
            "Iteration 729, loss = 0.03079705\n",
            "Iteration 730, loss = 0.03077770\n",
            "Iteration 731, loss = 0.03068195\n",
            "Iteration 732, loss = 0.03037415\n",
            "Iteration 733, loss = 0.03020068\n",
            "Iteration 734, loss = 0.03016887\n",
            "Iteration 735, loss = 0.03015838\n",
            "Iteration 736, loss = 0.02999921\n",
            "Iteration 737, loss = 0.02936837\n",
            "Iteration 738, loss = 0.02981704\n",
            "Iteration 739, loss = 0.02961027\n",
            "Iteration 740, loss = 0.02940959\n",
            "Iteration 741, loss = 0.02905345\n",
            "Iteration 742, loss = 0.02923713\n",
            "Iteration 743, loss = 0.02913475\n",
            "Iteration 744, loss = 0.02901290\n",
            "Iteration 745, loss = 0.02902492\n",
            "Iteration 746, loss = 0.02894906\n",
            "Iteration 747, loss = 0.02868140\n",
            "Iteration 748, loss = 0.02886651\n",
            "Iteration 749, loss = 0.02848454\n",
            "Iteration 750, loss = 0.02882210\n",
            "Iteration 751, loss = 0.02819992\n",
            "Iteration 752, loss = 0.02802275\n",
            "Iteration 753, loss = 0.02802081\n",
            "Iteration 754, loss = 0.02812014\n",
            "Iteration 755, loss = 0.02783947\n",
            "Iteration 756, loss = 0.02795773\n",
            "Iteration 757, loss = 0.02798393\n",
            "Iteration 758, loss = 0.02750563\n",
            "Iteration 759, loss = 0.02766674\n",
            "Iteration 760, loss = 0.02765267\n",
            "Iteration 761, loss = 0.02746319\n",
            "Iteration 762, loss = 0.02786864\n",
            "Iteration 763, loss = 0.02734624\n",
            "Iteration 764, loss = 0.02712494\n",
            "Iteration 765, loss = 0.02725337\n",
            "Iteration 766, loss = 0.02721773\n",
            "Iteration 767, loss = 0.02696614\n",
            "Iteration 768, loss = 0.02675882\n",
            "Iteration 769, loss = 0.02697299\n",
            "Iteration 770, loss = 0.02653169\n",
            "Iteration 771, loss = 0.02655563\n",
            "Iteration 772, loss = 0.02624956\n",
            "Iteration 773, loss = 0.02626846\n",
            "Iteration 774, loss = 0.02624944\n",
            "Iteration 775, loss = 0.02601873\n",
            "Iteration 776, loss = 0.02598884\n",
            "Iteration 777, loss = 0.02580545\n",
            "Iteration 778, loss = 0.02585655\n",
            "Iteration 779, loss = 0.02576797\n",
            "Iteration 780, loss = 0.02588354\n",
            "Iteration 781, loss = 0.02604809\n",
            "Iteration 782, loss = 0.02588497\n",
            "Iteration 783, loss = 0.02575618\n",
            "Iteration 784, loss = 0.02551792\n",
            "Iteration 785, loss = 0.02551834\n",
            "Iteration 786, loss = 0.02536926\n",
            "Iteration 787, loss = 0.02526187\n",
            "Iteration 788, loss = 0.02524302\n",
            "Iteration 789, loss = 0.02523065\n",
            "Iteration 790, loss = 0.02508836\n",
            "Iteration 791, loss = 0.02479876\n",
            "Iteration 792, loss = 0.02472155\n",
            "Iteration 793, loss = 0.02453940\n",
            "Iteration 794, loss = 0.02466521\n",
            "Iteration 795, loss = 0.02444052\n",
            "Iteration 796, loss = 0.02450373\n",
            "Iteration 797, loss = 0.02438213\n",
            "Iteration 798, loss = 0.02458483\n",
            "Iteration 799, loss = 0.02417802\n",
            "Iteration 800, loss = 0.02464177\n",
            "Iteration 801, loss = 0.02451401\n",
            "Iteration 802, loss = 0.02404101\n",
            "Iteration 803, loss = 0.02409902\n",
            "Iteration 804, loss = 0.02392546\n",
            "Iteration 805, loss = 0.02365023\n",
            "Iteration 806, loss = 0.02400578\n",
            "Iteration 807, loss = 0.02387385\n",
            "Iteration 808, loss = 0.02363630\n",
            "Iteration 809, loss = 0.02371512\n",
            "Iteration 810, loss = 0.02356385\n",
            "Iteration 811, loss = 0.02335971\n",
            "Iteration 812, loss = 0.02345322\n",
            "Iteration 813, loss = 0.02338764\n",
            "Iteration 814, loss = 0.02351211\n",
            "Iteration 815, loss = 0.02334282\n",
            "Iteration 816, loss = 0.02291689\n",
            "Iteration 817, loss = 0.02323714\n",
            "Iteration 818, loss = 0.02285258\n",
            "Iteration 819, loss = 0.02287285\n",
            "Iteration 820, loss = 0.02290995\n",
            "Iteration 821, loss = 0.02262102\n",
            "Iteration 822, loss = 0.02325447\n",
            "Iteration 823, loss = 0.02325299\n",
            "Iteration 824, loss = 0.02263025\n",
            "Iteration 825, loss = 0.02252849\n",
            "Iteration 826, loss = 0.02253686\n",
            "Iteration 827, loss = 0.02248381\n",
            "Iteration 828, loss = 0.02236782\n",
            "Iteration 829, loss = 0.02241320\n",
            "Iteration 830, loss = 0.02223964\n",
            "Iteration 831, loss = 0.02208968\n",
            "Iteration 832, loss = 0.02226008\n",
            "Iteration 833, loss = 0.02228549\n",
            "Iteration 834, loss = 0.02212076\n",
            "Iteration 835, loss = 0.02193962\n",
            "Iteration 836, loss = 0.02177557\n",
            "Iteration 837, loss = 0.02198530\n",
            "Iteration 838, loss = 0.02178962\n",
            "Iteration 839, loss = 0.02174115\n",
            "Iteration 840, loss = 0.02187944\n",
            "Iteration 841, loss = 0.02145654\n",
            "Iteration 842, loss = 0.02168293\n",
            "Iteration 843, loss = 0.02154186\n",
            "Iteration 844, loss = 0.02139225\n",
            "Iteration 845, loss = 0.02129099\n",
            "Iteration 846, loss = 0.02136253\n",
            "Iteration 847, loss = 0.02141676\n",
            "Iteration 848, loss = 0.02121729\n",
            "Iteration 849, loss = 0.02123137\n",
            "Iteration 850, loss = 0.02128969\n",
            "Iteration 851, loss = 0.02102024\n",
            "Iteration 852, loss = 0.02117160\n",
            "Iteration 853, loss = 0.02102258\n",
            "Iteration 854, loss = 0.02130285\n",
            "Iteration 855, loss = 0.02093522\n",
            "Iteration 856, loss = 0.02085452\n",
            "Iteration 857, loss = 0.02077396\n",
            "Iteration 858, loss = 0.02088284\n",
            "Iteration 859, loss = 0.02054275\n",
            "Iteration 860, loss = 0.02073725\n",
            "Iteration 861, loss = 0.02074514\n",
            "Iteration 862, loss = 0.02038518\n",
            "Iteration 863, loss = 0.02057552\n",
            "Iteration 864, loss = 0.02047856\n",
            "Iteration 865, loss = 0.02031903\n",
            "Iteration 866, loss = 0.02022646\n",
            "Iteration 867, loss = 0.02027930\n",
            "Iteration 868, loss = 0.02028724\n",
            "Iteration 869, loss = 0.02048431\n",
            "Iteration 870, loss = 0.02044748\n",
            "Iteration 871, loss = 0.01999617\n",
            "Iteration 872, loss = 0.02050179\n",
            "Iteration 873, loss = 0.01994100\n",
            "Iteration 874, loss = 0.01990516\n",
            "Iteration 875, loss = 0.02015370\n",
            "Iteration 876, loss = 0.01979858\n",
            "Iteration 877, loss = 0.01985850\n",
            "Iteration 878, loss = 0.01968776\n",
            "Iteration 879, loss = 0.01963358\n",
            "Iteration 880, loss = 0.01983106\n",
            "Iteration 881, loss = 0.01956685\n",
            "Iteration 882, loss = 0.01955852\n",
            "Iteration 883, loss = 0.01952659\n",
            "Iteration 884, loss = 0.01937268\n",
            "Iteration 885, loss = 0.01957181\n",
            "Iteration 886, loss = 0.01962550\n",
            "Iteration 887, loss = 0.01938317\n",
            "Iteration 888, loss = 0.01943567\n",
            "Iteration 889, loss = 0.01934949\n",
            "Iteration 890, loss = 0.01924547\n",
            "Iteration 891, loss = 0.01967150\n",
            "Iteration 892, loss = 0.01936780\n",
            "Iteration 893, loss = 0.01918773\n",
            "Iteration 894, loss = 0.01924188\n",
            "Iteration 895, loss = 0.01920237\n",
            "Iteration 896, loss = 0.01898888\n",
            "Iteration 897, loss = 0.01887716\n",
            "Iteration 898, loss = 0.01877895\n",
            "Iteration 899, loss = 0.01885589\n",
            "Iteration 900, loss = 0.01890554\n",
            "Iteration 901, loss = 0.01879365\n",
            "Iteration 902, loss = 0.01862841\n",
            "Iteration 903, loss = 0.01869125\n",
            "Iteration 904, loss = 0.01893955\n",
            "Iteration 905, loss = 0.01897218\n",
            "Iteration 906, loss = 0.01868216\n",
            "Iteration 907, loss = 0.01845759\n",
            "Iteration 908, loss = 0.01840499\n",
            "Iteration 909, loss = 0.01882745\n",
            "Iteration 910, loss = 0.01859525\n",
            "Iteration 911, loss = 0.01834738\n",
            "Iteration 912, loss = 0.01838582\n",
            "Iteration 913, loss = 0.01828258\n",
            "Iteration 914, loss = 0.01827858\n",
            "Iteration 915, loss = 0.01800447\n",
            "Iteration 916, loss = 0.01814189\n",
            "Iteration 917, loss = 0.01809108\n",
            "Iteration 918, loss = 0.01815592\n",
            "Iteration 919, loss = 0.01829162\n",
            "Iteration 920, loss = 0.01804480\n",
            "Iteration 921, loss = 0.01801889\n",
            "Iteration 922, loss = 0.01788856\n",
            "Iteration 923, loss = 0.01776753\n",
            "Iteration 924, loss = 0.01791956\n",
            "Iteration 925, loss = 0.01798908\n",
            "Iteration 926, loss = 0.01790512\n",
            "Iteration 927, loss = 0.01765069\n",
            "Iteration 928, loss = 0.01785193\n",
            "Iteration 929, loss = 0.01766150\n",
            "Iteration 930, loss = 0.01792725\n",
            "Iteration 931, loss = 0.01764981\n",
            "Iteration 932, loss = 0.01758548\n",
            "Iteration 933, loss = 0.01760028\n",
            "Iteration 934, loss = 0.01752720\n",
            "Iteration 935, loss = 0.01743561\n",
            "Iteration 936, loss = 0.01758002\n",
            "Iteration 937, loss = 0.01730674\n",
            "Iteration 938, loss = 0.01739042\n",
            "Iteration 939, loss = 0.01720992\n",
            "Iteration 940, loss = 0.01727452\n",
            "Iteration 941, loss = 0.01725710\n",
            "Iteration 942, loss = 0.01732147\n",
            "Iteration 943, loss = 0.01733271\n",
            "Iteration 944, loss = 0.01716886\n",
            "Iteration 945, loss = 0.01703015\n",
            "Iteration 946, loss = 0.01716348\n",
            "Iteration 947, loss = 0.01699144\n",
            "Iteration 948, loss = 0.01686027\n",
            "Iteration 949, loss = 0.01749480\n",
            "Iteration 950, loss = 0.01708333\n",
            "Iteration 951, loss = 0.01694680\n",
            "Iteration 952, loss = 0.01682356\n",
            "Iteration 953, loss = 0.01697129\n",
            "Iteration 954, loss = 0.01698586\n",
            "Iteration 955, loss = 0.01670127\n",
            "Iteration 956, loss = 0.01675512\n",
            "Iteration 957, loss = 0.01666091\n",
            "Iteration 958, loss = 0.01665452\n",
            "Iteration 959, loss = 0.01677495\n",
            "Iteration 960, loss = 0.01666775\n",
            "Iteration 961, loss = 0.01658872\n",
            "Iteration 962, loss = 0.01661232\n",
            "Iteration 963, loss = 0.01656419\n",
            "Iteration 964, loss = 0.01646468\n",
            "Iteration 965, loss = 0.01641381\n",
            "Iteration 966, loss = 0.01646968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 967, loss = 0.01623410\n",
            "Iteration 968, loss = 0.01606839\n",
            "Iteration 969, loss = 0.01589876\n",
            "Iteration 970, loss = 0.01582521\n",
            "Iteration 971, loss = 0.01576061\n",
            "Iteration 972, loss = 0.01571956\n",
            "Iteration 973, loss = 0.01572615\n",
            "Iteration 974, loss = 0.01570857\n",
            "Iteration 975, loss = 0.01567303\n",
            "Iteration 976, loss = 0.01570527\n",
            "Iteration 977, loss = 0.01572794\n",
            "Iteration 978, loss = 0.01571789\n",
            "Iteration 979, loss = 0.01567877\n",
            "Iteration 980, loss = 0.01567242\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 981, loss = 0.01555628\n",
            "Iteration 982, loss = 0.01555525\n",
            "Iteration 983, loss = 0.01554052\n",
            "Iteration 984, loss = 0.01552590\n",
            "Iteration 985, loss = 0.01553272\n",
            "Iteration 986, loss = 0.01552325\n",
            "Iteration 987, loss = 0.01552543\n",
            "Iteration 988, loss = 0.01552761\n",
            "Iteration 989, loss = 0.01552254\n",
            "Iteration 990, loss = 0.01552372\n",
            "Iteration 991, loss = 0.01552884\n",
            "Iteration 992, loss = 0.01552244\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 993, loss = 0.01549405\n",
            "Iteration 994, loss = 0.01549705\n",
            "Iteration 995, loss = 0.01549543\n",
            "Iteration 996, loss = 0.01549285\n",
            "Iteration 997, loss = 0.01549420\n",
            "Iteration 998, loss = 0.01549270\n",
            "Iteration 999, loss = 0.01549418\n",
            "Iteration 1000, loss = 0.01549235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(100, 100, 100), learning_rate='adaptive',\n",
              "              max_iter=1000, solver='sgd', verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Use your trained model to classify Intents"
      ],
      "metadata": {
        "id": "0Us_TnMB-qMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classify_intent = lambda text: map_intent_id_to_name(classifier.predict([embed_text(text)])[0])"
      ],
      "metadata": {
        "id": "OYjBzTW-3ul9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QodyGVFvWfQV",
        "outputId": "24169c05-27a1-4c2b-bc84-50c628231f2d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i need to know how to say hello in france'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_intents[110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pz8Ju8hBWhJz",
        "outputId": "f0bdf4ed-c304-4426-eea4-026f5ddfe06e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'translate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_intent(sentences[110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Mc4WrWehUo1o",
        "outputId": "41dd82b5-4517-4ec0-95b6-f2334c0d743a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'translate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Save your trained model"
      ],
      "metadata": {
        "id": "HYV8G6uRYEcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "PATH = \"intent_classifier.joblib.pkl\""
      ],
      "metadata": {
        "id": "0TKFl8rkYIhp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump(classifier, PATH, compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn7crj_1YeDJ",
        "outputId": "c7ec3f44-444b-491a-89ac-d1689dd9691a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['intent_classifier.joblib.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = load(PATH)"
      ],
      "metadata": {
        "id": "3UQUn7ZyYZzi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "plot(classifier.loss_curve_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "PGl3e5MoYw67",
        "outputId": "31872e58-3360-4288-8c66-f6680fccc841"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb6f63ddb10>]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_intent(sentences[110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "AWFzgDC4Yi7S",
        "outputId": "3b5522c7-1d25-4163-a088-8e5b4f89bdca"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'translate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}