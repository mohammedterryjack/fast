{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast import load\n",
    "\n",
    "tokeniser = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[117797, 117730, 117659, 105137]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokeniser.decode([117797, 117730, 117659, 105134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = this\n",
       "        morphology = this\n",
       "        phonology = 0S\n",
       "        id = 117797\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = is\n",
       "        morphology = is\n",
       "        phonology = IS\n",
       "        id = 117730\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = a\n",
       "        morphology = a\n",
       "        phonology = A\n",
       "        id = 117659\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = test\n",
       "        morphology = test\n",
       "        phonology = TST\n",
       "        id = 105134\n",
       "        tag = Verb\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'be.v.01'}\n",
       "        definition = show a certain characteristic when tested\n",
       "        example = He tested positive for HIV\n",
       "        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound words (e.g. 'big shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = the\n",
       "        morphology = the\n",
       "        phonology = 0\n",
       "        id = 117788\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = big shot\n",
       "        morphology = big shot\n",
       "        phonology = BK XT\n",
       "        id = 11031\n",
       "        tag = Noun\n",
       "        similar = {'big_shot', 'head_honcho', 'big_enchilada', 'big_gun', 'big_deal', 'big_wheel', 'big_cheese', 'big_fish'}\n",
       "        opposite = set()\n",
       "        related = {'supremo', 'knocker', 'colloquialism'}\n",
       "        semantics = {'whole.n.02', 'living_thing.n.01', 'object.n.01', 'person.n.01', 'important_person.n.01', 'organism.n.01', 'entity.n.01', 'adult.n.01', 'causal_agent.n.01', 'physical_entity.n.01'}\n",
       "        definition = an important influential person\n",
       "        example = he thinks he's a big shot; she's a big deal in local politics; the Qaeda commander is a very big fish\n",
       "        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"the big shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tokens (i.e. for Entity Extraction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now redfox'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"i will fly to the nasa space station now redfox\")\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_unknowns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station redfox'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasa space station'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.nouns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.verbs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (e.g. \"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fasting\n",
       "        morphology = fasting\n",
       "        phonology = FSTNK\n",
       "        id = 41354\n",
       "        tag = Adjective\n",
       "        similar = {'quick', 'fast', 'flying'}\n",
       "        opposite = set()\n",
       "        related = {'hurried'}\n",
       "        semantics = set()\n",
       "        definition = hurried and brief\n",
       "        example = paid a flying visit; took a flying glance at the book; a quick inspection; a fast visit\n",
       "        "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"are you fasting\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 41354\n",
       "        tag = Adjective\n",
       "        similar = {'quick', 'flying'}\n",
       "        opposite = set()\n",
       "        related = {'hurried'}\n",
       "        semantics = set()\n",
       "        definition = hurried and brief\n",
       "        example = paid a flying visit; took a flying glance at the book; a quick inspection; a fast visit\n",
       "        "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"i'm going too fast\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a big_shot \n",
      "he is a head_honcho \n",
      "he is a big_enchilada \n",
      "he is a big_gun \n",
      "he is a big_deal \n",
      "he is a big_wheel \n",
      "he is a big_cheese \n",
      "he is a big_fish \n",
      "he is a big shot\n"
     ]
    }
   ],
   "source": [
    "for variant in tokeniser.encode(\"he is a big shot\").paraphrase():\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Token Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = tuna\n",
       "        morphology = tuna\n",
       "        phonology = TN\n",
       "        id = 108939\n",
       "        tag = Noun\n",
       "        similar = {'Anguilla_sucklandii'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'bony_fish.n.01', 'living_thing.n.01', 'soft-finned_fish.n.01', 'animal.n.01', 'object.n.01', 'physical_entity.n.01', 'chordate.n.01', 'eel.n.02', 'vertebrate.n.01', 'organism.n.01', 'entity.n.01', 'whole.n.02', 'aquatic_vertebrate.n.01', 'teleost_fish.n.01', 'fish.n.01'}\n",
       "        definition = New Zealand eel\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna = tokeniser.encode(\"tuna\")[0]\n",
    "tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fish and chips\n",
       "        morphology = fish and chip\n",
       "        phonology = FX ANT XP\n",
       "        id = 40416\n",
       "        tag = Noun\n",
       "        similar = {'fish_and_chips'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'substance.n.07', 'nutriment.n.01', 'dish.n.02', 'food.n.01', 'matter.n.03', 'entity.n.01', 'physical_entity.n.01'}\n",
       "        definition = fried fish and french-fried potatoes\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokeniser.encode(\"i ate fish and chips on the weekend\")[:]\n",
    "tuna.most_similar(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Contextual Sentence Embeddings (dot similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90929743, 1.        , 0.90962228, ..., 4.0310051 , 1.94108177,\n",
       "       3.11605277])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an exam'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = outputs.most_similar([\n",
    "    tokeniser.encode(\"test this is\"),\n",
    "    tokeniser.encode(\"this is an exam\"),\n",
    "    tokeniser.encode(\"bla bla bla food\")\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London has 9,787,426 inhabitants at the 2011 census'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tokeniser.encode(\"How big is London\").most_similar([\n",
    "    tokeniser.encode(\"London has 9,787,426 inhabitants at the 2011 census\"),\n",
    "    tokeniser.encode(\"London is known for its financial district\"),\n",
    "])\n",
    "str(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testground",
   "language": "python",
   "name": "testground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
