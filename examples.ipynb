{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[117797, 117730, 117659, 105134]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokeniser.decode([117797, 117730, 117659, 105134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = this\n",
       "        morphology = this\n",
       "        phonology = 0S\n",
       "        id = 117797\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = is\n",
       "        morphology = is\n",
       "        phonology = IS\n",
       "        id = 117730\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = a\n",
       "        morphology = a\n",
       "        phonology = A\n",
       "        id = 117659\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = test\n",
       "        morphology = test\n",
       "        phonology = TST\n",
       "        id = 105134\n",
       "        tag = Verb\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'be.v.01'}\n",
       "        definition = show a certain characteristic when tested\n",
       "        example = He tested positive for HIV\n",
       "        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound words (e.g. 'big shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = the\n",
       "        morphology = the\n",
       "        phonology = 0\n",
       "        id = 117788\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = big shot\n",
       "        morphology = big shot\n",
       "        phonology = BK XT\n",
       "        id = 11031\n",
       "        tag = Noun\n",
       "        similar = {'big_cheese', 'big_deal', 'big_wheel', 'big_fish', 'big_enchilada', 'head_honcho', 'big_gun', 'big_shot'}\n",
       "        opposite = set()\n",
       "        related = {'knocker', 'colloquialism', 'supremo'}\n",
       "        semantics = {'causal_agent.n.01', 'important_person.n.01', 'living_thing.n.01', 'organism.n.01', 'entity.n.01', 'whole.n.02', 'person.n.01', 'physical_entity.n.01', 'object.n.01', 'adult.n.01'}\n",
       "        definition = an important influential person\n",
       "        example = he thinks he's a big shot; she's a big deal in local politics; the Qaeda commander is a very big fish\n",
       "        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"the big shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tokens (i.e. for Entity Extraction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am flying onto the nasa space station now redfox'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"i am flying onto the nasa space station now redfox\")\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am flying the nasa space station now'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_unknowns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flying onto nasa space station redfox'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasa space station'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.nouns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flying'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.verbs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flying nasa space station'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (e.g. \"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1 = tokeniser.encode(\"i like to fast from food\")\n",
    "outputs2 = tokeniser.encode(\"i'm going too fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 38987\n",
       "        tag = Adjective\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = {'causative'}\n",
       "        semantics = set()\n",
       "        definition = (of a photographic lens or emulsion) causing a shortening of exposure time\n",
       "        example = a fast lens\n",
       "        "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs1[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 38987\n",
       "        tag = Adjective\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = {'causative'}\n",
       "        semantics = set()\n",
       "        definition = (of a photographic lens or emulsion) causing a shortening of exposure time\n",
       "        example = a fast lens\n",
       "        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i taste a rat\n",
      "i smack a rat\n",
      "i smell a scab \n",
      "i smell a blackleg \n",
      "i smell a strikebreaker \n",
      "i smell a rat\n"
     ]
    }
   ],
   "source": [
    "for variant in tokeniser.encode(\"i smell a rat\").paraphrase():\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Token Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fish\n",
       "        morphology = fish\n",
       "        phonology = FX\n",
       "        id = 40414\n",
       "        tag = Noun\n",
       "        similar = {'Pisces', 'Fish'}\n",
       "        opposite = set()\n",
       "        related = {'star_divination', 'astrology'}\n",
       "        semantics = {'causal_agent.n.01', 'living_thing.n.01', 'organism.n.01', 'entity.n.01', 'whole.n.02', 'person.n.01', 'physical_entity.n.01', 'object.n.01'}\n",
       "        definition = (astrology) a person who is born while the sun is in Pisces\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish = tokeniser.encode(\"fish\")[0]\n",
    "fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = salmon\n",
       "        morphology = salmon\n",
       "        phonology = SLMN\n",
       "        id = 90581\n",
       "        tag = Noun\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'color.n.01', 'chromatic_color.n.01', 'entity.n.01', 'property.n.02', 'attribute.n.02', 'abstraction.n.06', 'visual_property.n.01'}\n",
       "        definition = a pale pinkish orange color\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokeniser.encode(\"i ate a salmon pie on the weekend\")[:]\n",
    "fish.most_similar(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Contextual Sentence Embeddings (dot similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90929743, 1.        , 0.90962228, ..., 4.03114547, 1.94133054,\n",
       "       3.11597023])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an exam'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = outputs.most_similar([\n",
    "    tokeniser.encode(\"test this is\"),\n",
    "    tokeniser.encode(\"this is an exam\"),\n",
    "    tokeniser.encode(\"bla bla bla food\")\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London has 9,787,426 inhabitants at the 2011 census'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tokeniser.encode(\"How big is London\").most_similar([\n",
    "    tokeniser.encode(\"London has 9,787,426 inhabitants at the 2011 census\"),\n",
    "    tokeniser.encode(\"London is known for its financial district\"),\n",
    "])\n",
    "str(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testground",
   "language": "python",
   "name": "testground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
