{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mohammedterry-\n",
      "[nltk_data]     jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mohammedterry-\n",
      "[nltk_data]     jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ffast import load\n",
    "\n",
    "tokeniser = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[117797, 117730, 117659, 105137]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.decode([117797, 117730, 117659, 105134])\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = this\n",
       "        morphology = this\n",
       "        phonology = 0S\n",
       "        id = 117797\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = is\n",
       "        morphology = is\n",
       "        phonology = IS\n",
       "        id = 117730\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = a\n",
       "        morphology = a\n",
       "        phonology = A\n",
       "        id = 117659\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = test\n",
       "        morphology = test\n",
       "        phonology = TST\n",
       "        id = 105134\n",
       "        tag = Verb\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'be.v.01'}\n",
       "        definition = show a certain characteristic when tested\n",
       "        example = He tested positive for HIV\n",
       "        "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound words (e.g. 'big shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = the\n",
       "        morphology = the\n",
       "        phonology = 0\n",
       "        id = 117788\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = big shot\n",
       "        morphology = big shot\n",
       "        phonology = BK XT\n",
       "        id = 11031\n",
       "        tag = Noun\n",
       "        similar = {'big_wheel', 'big_deal', 'big_shot', 'head_honcho', 'big_gun', 'big_cheese', 'big_enchilada', 'big_fish'}\n",
       "        opposite = set()\n",
       "        related = {'supremo', 'knocker', 'colloquialism'}\n",
       "        semantics = {'person.n.01', 'adult.n.01', 'entity.n.01', 'important_person.n.01', 'living_thing.n.01', 'whole.n.02', 'physical_entity.n.01', 'object.n.01', 'organism.n.01', 'causal_agent.n.01'}\n",
       "        definition = an important influential person\n",
       "        example = he thinks he's a big shot; she's a big deal in local politics; the Qaeda commander is a very big fish\n",
       "        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"the big shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tokens (i.e. for Entity Extraction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now redfox'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"i will fly to the nasa space station now redfox\")\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'will', 'fly', 'to', 'the', 'nasa', 'space station', 'now', 'redfox']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str,outputs.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_unknowns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station redfox'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasa space station'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.nouns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.verbs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station redfox'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (e.g. \"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 38980\n",
       "        tag = Noun\n",
       "        similar = {'fasting'}\n",
       "        opposite = set()\n",
       "        related = {'Ramadan', 'diet', 'hunger_strike', 'dieting'}\n",
       "        semantics = {'entity.n.01', 'event.n.01', 'self-denial.n.02', 'psychological_feature.n.01', 'control.n.05', 'abstinence.n.02', 'act.n.02', 'activity.n.01', 'abstraction.n.06'}\n",
       "        definition = abstaining from food\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"fast from food in Ramadan\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 41354\n",
       "        tag = Adjective\n",
       "        similar = {'quick', 'flying'}\n",
       "        opposite = set()\n",
       "        related = {'hurried'}\n",
       "        semantics = set()\n",
       "        definition = hurried and brief\n",
       "        example = paid a flying visit; took a flying glance at the book; a quick inspection; a fast visit\n",
       "        "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"i'm going too fast\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a big_wheel \n",
      "he is a big_deal \n",
      "he is a big_shot \n",
      "he is a head_honcho \n",
      "he is a big_gun \n",
      "he is a big_cheese \n",
      "he is a big_enchilada \n",
      "he is a big_fish \n",
      "he is a big shot\n"
     ]
    }
   ],
   "source": [
    "for variant in tokeniser.encode(\"he is a big shot\").paraphrase():\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Token Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = tuna\n",
       "        morphology = tuna\n",
       "        phonology = TN\n",
       "        id = 108939\n",
       "        tag = Noun\n",
       "        similar = {'Anguilla_sucklandii'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'entity.n.01', 'teleost_fish.n.01', 'living_thing.n.01', 'whole.n.02', 'animal.n.01', 'chordate.n.01', 'physical_entity.n.01', 'fish.n.01', 'soft-finned_fish.n.01', 'eel.n.02', 'vertebrate.n.01', 'bony_fish.n.01', 'object.n.01', 'aquatic_vertebrate.n.01', 'organism.n.01'}\n",
       "        definition = New Zealand eel\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna = tokeniser.encode(\"tuna\")[0]\n",
    "tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fish and chips\n",
       "        morphology = fish and chip\n",
       "        phonology = FX ANT XP\n",
       "        id = 40416\n",
       "        tag = Noun\n",
       "        similar = {'fish_and_chips'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'entity.n.01', 'matter.n.03', 'physical_entity.n.01', 'dish.n.02', 'substance.n.07', 'food.n.01', 'nutriment.n.01'}\n",
       "        definition = fried fish and french-fried potatoes\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokeniser.encode(\"i ate fish and chips on the weekend\")[:]\n",
    "tuna.most_similar(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Contextual Sentence Embeddings (dot similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178620,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an exam'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = outputs.most_similar([\n",
    "    tokeniser.encode(\"test this is\"),\n",
    "    tokeniser.encode(\"this is an exam\"),\n",
    "    tokeniser.encode(\"bla bla bla food\")\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London has 9,787,426 inhabitants at the 2011 census'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tokeniser.encode(\"How big is London\").most_similar([\n",
    "    tokeniser.encode(\"London has 9,787,426 inhabitants at the 2011 census\"),\n",
    "    tokeniser.encode(\"London is known for its financial district\"),\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want smaller vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.projection().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d9e5edf0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow([outputs.projection()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testground",
   "language": "python",
   "name": "testground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
