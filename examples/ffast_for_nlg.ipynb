{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFast for NLG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Get some training data"
      ],
      "metadata": {
        "id": "mFMJBV2nStUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "3lGT4EEeAzOh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import genesis\n",
        "from nltk import download\n",
        "download('genesis')\n",
        "download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJovm_SzJ46u",
        "outputId": "c637b7bb-7b43-4692-9ad3-dfa01a16a283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(genesis.sents())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF2w52dlJ6xA",
        "outputId": "3f935515-6359-4138-86cf-f77c94168580"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13640"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "  for tokens in genesis.sents()[:100]:\n",
        "    for index in range(1,len(tokens)):\n",
        "      yield tokens[:index],tokens[index]"
      ],
      "metadata": {
        "id": "7TrDwNT67Hwp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = zip(*prepare_data())"
      ],
      "metadata": {
        "id": "VT7q3gyh8t28"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "634F2irDMuuP",
        "outputId": "968d6cba-db61-4bea-b135-6d73605f2bd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In', 'the', 'beginning', 'God']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "560YuLtGM74z",
        "outputId": "446b2c11-170d-4b9a-a4ad-08b3939a76d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'created'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Encode the data using FFast"
      ],
      "metadata": {
        "id": "oJjPpOVvSyGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Y5fR1P5hqf",
        "outputId": "204c7fa7-e3c8-4c6f-f256-8c1decd2078d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffast\n",
            "  Downloading ffast-0.1.6-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ffast) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.21.5)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.4.1)\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ffast) (1.15.0)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73990 sha256=a938d1cf1121f30fae04f7f43cbc4f71e795d8688b9b826d6308c6efff3411ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: Unidecode, jellyfish, ffast\n",
            "Successfully installed Unidecode-1.3.4 ffast-0.1.6 jellyfish-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ffast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ffast import load"
      ],
      "metadata": {
        "id": "d1MK1Q0z6GgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e719a16-b625-4350-9807-219636db2d97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokeniser = load(\"poincare\")\n",
        "output_tokeniser = load(\"wordnet\")"
      ],
      "metadata": {
        "id": "b8f-C_4M6LKh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = list(map(lambda tokens:input_tokeniser.encode(' '.join(tokens)).vector,x))\n",
        "y_train = list(map(lambda token:output_tokeniser.encode(token).ids[0], y))"
      ],
      "metadata": {
        "id": "CEOTUHdXMS-W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Tran a model"
      ],
      "metadata": {
        "id": "n7NVHG0FS68v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "classifier = MLPClassifier(\n",
        "    hidden_layer_sizes=(512)*3, \n",
        "    solver='sgd',\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=1000,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZylJfmQe8wzg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L5JZtlY9EQ-",
        "outputId": "dcc5643e-4bf4-4a75-e51f-a45d91961b43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 17.07568361\n",
            "Iteration 2, loss = 6.16359652\n",
            "Iteration 3, loss = 4.69774592\n",
            "Iteration 4, loss = 4.48478637\n",
            "Iteration 5, loss = 4.36463575\n",
            "Iteration 6, loss = 4.29740873\n",
            "Iteration 7, loss = 4.25145588\n",
            "Iteration 8, loss = 4.22576051\n",
            "Iteration 9, loss = 4.19560385\n",
            "Iteration 10, loss = 4.17907644\n",
            "Iteration 11, loss = 4.15756432\n",
            "Iteration 12, loss = 4.13972642\n",
            "Iteration 13, loss = 4.12759948\n",
            "Iteration 14, loss = 4.11541783\n",
            "Iteration 15, loss = 4.09982496\n",
            "Iteration 16, loss = 4.08658337\n",
            "Iteration 17, loss = 4.07424441\n",
            "Iteration 18, loss = 4.06790857\n",
            "Iteration 19, loss = 4.05443347\n",
            "Iteration 20, loss = 4.03981798\n",
            "Iteration 21, loss = 4.02850792\n",
            "Iteration 22, loss = 4.02545225\n",
            "Iteration 23, loss = 4.00701189\n",
            "Iteration 24, loss = 3.99962220\n",
            "Iteration 25, loss = 3.98812812\n",
            "Iteration 26, loss = 3.97512794\n",
            "Iteration 27, loss = 3.96578174\n",
            "Iteration 28, loss = 3.95942631\n",
            "Iteration 29, loss = 3.95105031\n",
            "Iteration 30, loss = 3.94064904\n",
            "Iteration 31, loss = 3.92826976\n",
            "Iteration 32, loss = 3.91896603\n",
            "Iteration 33, loss = 3.90449335\n",
            "Iteration 34, loss = 3.89549744\n",
            "Iteration 35, loss = 3.89036709\n",
            "Iteration 36, loss = 3.88076441\n",
            "Iteration 37, loss = 3.87486431\n",
            "Iteration 38, loss = 3.85814652\n",
            "Iteration 39, loss = 3.85058381\n",
            "Iteration 40, loss = 3.84523838\n",
            "Iteration 41, loss = 3.83406474\n",
            "Iteration 42, loss = 3.82104946\n",
            "Iteration 43, loss = 3.82979789\n",
            "Iteration 44, loss = 3.80651724\n",
            "Iteration 45, loss = 3.81164934\n",
            "Iteration 46, loss = 3.78687861\n",
            "Iteration 47, loss = 3.78158441\n",
            "Iteration 48, loss = 3.77333119\n",
            "Iteration 49, loss = 3.76776235\n",
            "Iteration 50, loss = 3.75643291\n",
            "Iteration 51, loss = 3.74440179\n",
            "Iteration 52, loss = 3.73449806\n",
            "Iteration 53, loss = 3.72549876\n",
            "Iteration 54, loss = 3.71216101\n",
            "Iteration 55, loss = 3.70507839\n",
            "Iteration 56, loss = 3.69550357\n",
            "Iteration 57, loss = 3.68818746\n",
            "Iteration 58, loss = 3.69348146\n",
            "Iteration 59, loss = 3.67076826\n",
            "Iteration 60, loss = 3.66396506\n",
            "Iteration 61, loss = 3.65542843\n",
            "Iteration 62, loss = 3.64159343\n",
            "Iteration 63, loss = 3.63547803\n",
            "Iteration 64, loss = 3.63328137\n",
            "Iteration 65, loss = 3.60938035\n",
            "Iteration 66, loss = 3.62049630\n",
            "Iteration 67, loss = 3.59400479\n",
            "Iteration 68, loss = 3.59459213\n",
            "Iteration 69, loss = 3.58837841\n",
            "Iteration 70, loss = 3.57027226\n",
            "Iteration 71, loss = 3.56496095\n",
            "Iteration 72, loss = 3.56123968\n",
            "Iteration 73, loss = 3.54425066\n",
            "Iteration 74, loss = 3.53765915\n",
            "Iteration 75, loss = 3.53256699\n",
            "Iteration 76, loss = 3.51437407\n",
            "Iteration 77, loss = 3.51490867\n",
            "Iteration 78, loss = 3.49648291\n",
            "Iteration 79, loss = 3.48829178\n",
            "Iteration 80, loss = 3.48532268\n",
            "Iteration 81, loss = 3.47605638\n",
            "Iteration 82, loss = 3.46372237\n",
            "Iteration 83, loss = 3.45781067\n",
            "Iteration 84, loss = 3.45230121\n",
            "Iteration 85, loss = 3.44844814\n",
            "Iteration 86, loss = 3.42735399\n",
            "Iteration 87, loss = 3.43126188\n",
            "Iteration 88, loss = 3.41729866\n",
            "Iteration 89, loss = 3.40202791\n",
            "Iteration 90, loss = 3.39774953\n",
            "Iteration 91, loss = 3.37858918\n",
            "Iteration 92, loss = 3.37539738\n",
            "Iteration 93, loss = 3.37325138\n",
            "Iteration 94, loss = 3.37194446\n",
            "Iteration 95, loss = 3.34892152\n",
            "Iteration 96, loss = 3.34334973\n",
            "Iteration 97, loss = 3.33478638\n",
            "Iteration 98, loss = 3.33353558\n",
            "Iteration 99, loss = 3.31748485\n",
            "Iteration 100, loss = 3.31860156\n",
            "Iteration 101, loss = 3.30132497\n",
            "Iteration 102, loss = 3.28938324\n",
            "Iteration 103, loss = 3.29220465\n",
            "Iteration 104, loss = 3.27436189\n",
            "Iteration 105, loss = 3.27022155\n",
            "Iteration 106, loss = 3.25512778\n",
            "Iteration 107, loss = 3.25903901\n",
            "Iteration 108, loss = 3.24314321\n",
            "Iteration 109, loss = 3.23404916\n",
            "Iteration 110, loss = 3.23153961\n",
            "Iteration 111, loss = 3.21410088\n",
            "Iteration 112, loss = 3.20664264\n",
            "Iteration 113, loss = 3.21403921\n",
            "Iteration 114, loss = 3.19832725\n",
            "Iteration 115, loss = 3.20368587\n",
            "Iteration 116, loss = 3.18627991\n",
            "Iteration 117, loss = 3.17121847\n",
            "Iteration 118, loss = 3.16613001\n",
            "Iteration 119, loss = 3.16228798\n",
            "Iteration 120, loss = 3.16516897\n",
            "Iteration 121, loss = 3.13069222\n",
            "Iteration 122, loss = 3.12455833\n",
            "Iteration 123, loss = 3.12464515\n",
            "Iteration 124, loss = 3.12161823\n",
            "Iteration 125, loss = 3.11790280\n",
            "Iteration 126, loss = 3.10431801\n",
            "Iteration 127, loss = 3.08913576\n",
            "Iteration 128, loss = 3.07447707\n",
            "Iteration 129, loss = 3.08417019\n",
            "Iteration 130, loss = 3.06923390\n",
            "Iteration 131, loss = 3.06080752\n",
            "Iteration 132, loss = 3.05134850\n",
            "Iteration 133, loss = 3.04098032\n",
            "Iteration 134, loss = 3.03209608\n",
            "Iteration 135, loss = 3.03662410\n",
            "Iteration 136, loss = 3.02617146\n",
            "Iteration 137, loss = 3.01756174\n",
            "Iteration 138, loss = 3.00285429\n",
            "Iteration 139, loss = 3.00185666\n",
            "Iteration 140, loss = 2.98434290\n",
            "Iteration 141, loss = 2.98084913\n",
            "Iteration 142, loss = 2.98588862\n",
            "Iteration 143, loss = 2.96219493\n",
            "Iteration 144, loss = 2.95696250\n",
            "Iteration 145, loss = 2.96134386\n",
            "Iteration 146, loss = 2.96033423\n",
            "Iteration 147, loss = 2.93706457\n",
            "Iteration 148, loss = 2.93477031\n",
            "Iteration 149, loss = 2.92340315\n",
            "Iteration 150, loss = 2.92547480\n",
            "Iteration 151, loss = 2.89520011\n",
            "Iteration 152, loss = 2.89255160\n",
            "Iteration 153, loss = 2.89167098\n",
            "Iteration 154, loss = 2.89362616\n",
            "Iteration 155, loss = 2.89197233\n",
            "Iteration 156, loss = 2.85557589\n",
            "Iteration 157, loss = 2.86453091\n",
            "Iteration 158, loss = 2.85759174\n",
            "Iteration 159, loss = 2.85823237\n",
            "Iteration 160, loss = 2.82930479\n",
            "Iteration 161, loss = 2.82904558\n",
            "Iteration 162, loss = 2.83950118\n",
            "Iteration 163, loss = 2.81614112\n",
            "Iteration 164, loss = 2.81500927\n",
            "Iteration 165, loss = 2.80096009\n",
            "Iteration 166, loss = 2.78347862\n",
            "Iteration 167, loss = 2.79062559\n",
            "Iteration 168, loss = 2.79111987\n",
            "Iteration 169, loss = 2.79564070\n",
            "Iteration 170, loss = 2.76047110\n",
            "Iteration 171, loss = 2.75893941\n",
            "Iteration 172, loss = 2.75667449\n",
            "Iteration 173, loss = 2.73881577\n",
            "Iteration 174, loss = 2.72907780\n",
            "Iteration 175, loss = 2.73972918\n",
            "Iteration 176, loss = 2.71788389\n",
            "Iteration 177, loss = 2.70870199\n",
            "Iteration 178, loss = 2.73210378\n",
            "Iteration 179, loss = 2.70847482\n",
            "Iteration 180, loss = 2.68500012\n",
            "Iteration 181, loss = 2.70161978\n",
            "Iteration 182, loss = 2.66760110\n",
            "Iteration 183, loss = 2.66501678\n",
            "Iteration 184, loss = 2.67914418\n",
            "Iteration 185, loss = 2.66162117\n",
            "Iteration 186, loss = 2.63843001\n",
            "Iteration 187, loss = 2.64021645\n",
            "Iteration 188, loss = 2.65073345\n",
            "Iteration 189, loss = 2.63615308\n",
            "Iteration 190, loss = 2.62529983\n",
            "Iteration 191, loss = 2.60593923\n",
            "Iteration 192, loss = 2.59393000\n",
            "Iteration 193, loss = 2.62109196\n",
            "Iteration 194, loss = 2.57682335\n",
            "Iteration 195, loss = 2.58650140\n",
            "Iteration 196, loss = 2.57934528\n",
            "Iteration 197, loss = 2.56373639\n",
            "Iteration 198, loss = 2.57630578\n",
            "Iteration 199, loss = 2.54551813\n",
            "Iteration 200, loss = 2.55597095\n",
            "Iteration 201, loss = 2.54281494\n",
            "Iteration 202, loss = 2.55164820\n",
            "Iteration 203, loss = 2.52808704\n",
            "Iteration 204, loss = 2.51455518\n",
            "Iteration 205, loss = 2.50923109\n",
            "Iteration 206, loss = 2.50975009\n",
            "Iteration 207, loss = 2.49809267\n",
            "Iteration 208, loss = 2.52521900\n",
            "Iteration 209, loss = 2.47688277\n",
            "Iteration 210, loss = 2.49932727\n",
            "Iteration 211, loss = 2.52016940\n",
            "Iteration 212, loss = 2.48563621\n",
            "Iteration 213, loss = 2.45872392\n",
            "Iteration 214, loss = 2.45088789\n",
            "Iteration 215, loss = 2.46677066\n",
            "Iteration 216, loss = 2.43410702\n",
            "Iteration 217, loss = 2.42611580\n",
            "Iteration 218, loss = 2.42387985\n",
            "Iteration 219, loss = 2.46321238\n",
            "Iteration 220, loss = 2.44438855\n",
            "Iteration 221, loss = 2.41821041\n",
            "Iteration 222, loss = 2.40302702\n",
            "Iteration 223, loss = 2.40959305\n",
            "Iteration 224, loss = 2.38168264\n",
            "Iteration 225, loss = 2.37675677\n",
            "Iteration 226, loss = 2.36194904\n",
            "Iteration 227, loss = 2.36696242\n",
            "Iteration 228, loss = 2.36824701\n",
            "Iteration 229, loss = 2.35210697\n",
            "Iteration 230, loss = 2.35064102\n",
            "Iteration 231, loss = 2.37376906\n",
            "Iteration 232, loss = 2.33225158\n",
            "Iteration 233, loss = 2.31701957\n",
            "Iteration 234, loss = 2.30617013\n",
            "Iteration 235, loss = 2.31425321\n",
            "Iteration 236, loss = 2.34697967\n",
            "Iteration 237, loss = 2.29991153\n",
            "Iteration 238, loss = 2.32569702\n",
            "Iteration 239, loss = 2.28590344\n",
            "Iteration 240, loss = 2.25236497\n",
            "Iteration 241, loss = 2.29628197\n",
            "Iteration 242, loss = 2.26301961\n",
            "Iteration 243, loss = 2.24682056\n",
            "Iteration 244, loss = 2.24766719\n",
            "Iteration 245, loss = 2.30340341\n",
            "Iteration 246, loss = 2.22615720\n",
            "Iteration 247, loss = 2.25351601\n",
            "Iteration 248, loss = 2.24084170\n",
            "Iteration 249, loss = 2.20895434\n",
            "Iteration 250, loss = 2.21216491\n",
            "Iteration 251, loss = 2.23570616\n",
            "Iteration 252, loss = 2.21961031\n",
            "Iteration 253, loss = 2.18344919\n",
            "Iteration 254, loss = 2.17616499\n",
            "Iteration 255, loss = 2.21958454\n",
            "Iteration 256, loss = 2.17799604\n",
            "Iteration 257, loss = 2.18867964\n",
            "Iteration 258, loss = 2.15879245\n",
            "Iteration 259, loss = 2.15598288\n",
            "Iteration 260, loss = 2.17655031\n",
            "Iteration 261, loss = 2.12611323\n",
            "Iteration 262, loss = 2.20194064\n",
            "Iteration 263, loss = 2.17326074\n",
            "Iteration 264, loss = 2.13431082\n",
            "Iteration 265, loss = 2.15238655\n",
            "Iteration 266, loss = 2.15181946\n",
            "Iteration 267, loss = 2.15100557\n",
            "Iteration 268, loss = 2.09490917\n",
            "Iteration 269, loss = 2.09548653\n",
            "Iteration 270, loss = 2.11169224\n",
            "Iteration 271, loss = 2.08815245\n",
            "Iteration 272, loss = 2.08921471\n",
            "Iteration 273, loss = 2.06042379\n",
            "Iteration 274, loss = 2.08042563\n",
            "Iteration 275, loss = 2.04690292\n",
            "Iteration 276, loss = 2.04099326\n",
            "Iteration 277, loss = 2.03133645\n",
            "Iteration 278, loss = 2.02669085\n",
            "Iteration 279, loss = 2.04392184\n",
            "Iteration 280, loss = 2.03663861\n",
            "Iteration 281, loss = 1.99423143\n",
            "Iteration 282, loss = 2.00453962\n",
            "Iteration 283, loss = 2.03122682\n",
            "Iteration 284, loss = 2.00656000\n",
            "Iteration 285, loss = 2.00556899\n",
            "Iteration 286, loss = 1.97217232\n",
            "Iteration 287, loss = 2.05601036\n",
            "Iteration 288, loss = 1.96477916\n",
            "Iteration 289, loss = 1.96228934\n",
            "Iteration 290, loss = 1.95878106\n",
            "Iteration 291, loss = 1.95518086\n",
            "Iteration 292, loss = 1.95312466\n",
            "Iteration 293, loss = 1.93277807\n",
            "Iteration 294, loss = 1.93992116\n",
            "Iteration 295, loss = 1.98226195\n",
            "Iteration 296, loss = 1.95256702\n",
            "Iteration 297, loss = 1.98308967\n",
            "Iteration 298, loss = 1.94656848\n",
            "Iteration 299, loss = 1.88422552\n",
            "Iteration 300, loss = 1.88681990\n",
            "Iteration 301, loss = 1.90764551\n",
            "Iteration 302, loss = 1.90636572\n",
            "Iteration 303, loss = 1.87581597\n",
            "Iteration 304, loss = 1.88604535\n",
            "Iteration 305, loss = 1.92254557\n",
            "Iteration 306, loss = 1.86636033\n",
            "Iteration 307, loss = 1.86204107\n",
            "Iteration 308, loss = 1.90811188\n",
            "Iteration 309, loss = 1.86226467\n",
            "Iteration 310, loss = 1.88110166\n",
            "Iteration 311, loss = 1.84986360\n",
            "Iteration 312, loss = 1.85062011\n",
            "Iteration 313, loss = 1.86302066\n",
            "Iteration 314, loss = 1.85422828\n",
            "Iteration 315, loss = 1.80433100\n",
            "Iteration 316, loss = 1.81889547\n",
            "Iteration 317, loss = 1.84971950\n",
            "Iteration 318, loss = 1.81207673\n",
            "Iteration 319, loss = 1.83513693\n",
            "Iteration 320, loss = 1.80362725\n",
            "Iteration 321, loss = 1.78923267\n",
            "Iteration 322, loss = 1.78073829\n",
            "Iteration 323, loss = 1.76243743\n",
            "Iteration 324, loss = 1.74859460\n",
            "Iteration 325, loss = 1.82017640\n",
            "Iteration 326, loss = 1.73638919\n",
            "Iteration 327, loss = 1.75108345\n",
            "Iteration 328, loss = 1.78325490\n",
            "Iteration 329, loss = 1.73233456\n",
            "Iteration 330, loss = 1.74804239\n",
            "Iteration 331, loss = 1.75018270\n",
            "Iteration 332, loss = 1.73340950\n",
            "Iteration 333, loss = 1.74440820\n",
            "Iteration 334, loss = 1.71604922\n",
            "Iteration 335, loss = 1.67447803\n",
            "Iteration 336, loss = 1.70062109\n",
            "Iteration 337, loss = 1.69102908\n",
            "Iteration 338, loss = 1.66607074\n",
            "Iteration 339, loss = 1.66778721\n",
            "Iteration 340, loss = 1.72743188\n",
            "Iteration 341, loss = 1.64501140\n",
            "Iteration 342, loss = 1.70532646\n",
            "Iteration 343, loss = 1.69912066\n",
            "Iteration 344, loss = 1.64958706\n",
            "Iteration 345, loss = 1.64535408\n",
            "Iteration 346, loss = 1.65549343\n",
            "Iteration 347, loss = 1.64821515\n",
            "Iteration 348, loss = 1.61983325\n",
            "Iteration 349, loss = 1.64499605\n",
            "Iteration 350, loss = 1.61487196\n",
            "Iteration 351, loss = 1.64105873\n",
            "Iteration 352, loss = 1.63184363\n",
            "Iteration 353, loss = 1.66642071\n",
            "Iteration 354, loss = 1.66445345\n",
            "Iteration 355, loss = 1.60518009\n",
            "Iteration 356, loss = 1.58847988\n",
            "Iteration 357, loss = 1.62152501\n",
            "Iteration 358, loss = 1.61828778\n",
            "Iteration 359, loss = 1.61840965\n",
            "Iteration 360, loss = 1.61369088\n",
            "Iteration 361, loss = 1.57014007\n",
            "Iteration 362, loss = 1.56173129\n",
            "Iteration 363, loss = 1.52053315\n",
            "Iteration 364, loss = 1.56492778\n",
            "Iteration 365, loss = 1.59113482\n",
            "Iteration 366, loss = 1.52479057\n",
            "Iteration 367, loss = 1.57110900\n",
            "Iteration 368, loss = 1.52332301\n",
            "Iteration 369, loss = 1.53768472\n",
            "Iteration 370, loss = 1.55879856\n",
            "Iteration 371, loss = 1.55181780\n",
            "Iteration 372, loss = 1.52949355\n",
            "Iteration 373, loss = 1.50771630\n",
            "Iteration 374, loss = 1.47675534\n",
            "Iteration 375, loss = 1.52704404\n",
            "Iteration 376, loss = 1.48833544\n",
            "Iteration 377, loss = 1.50066254\n",
            "Iteration 378, loss = 1.49602028\n",
            "Iteration 379, loss = 1.50679109\n",
            "Iteration 380, loss = 1.48631815\n",
            "Iteration 381, loss = 1.43583500\n",
            "Iteration 382, loss = 1.46318398\n",
            "Iteration 383, loss = 1.45696422\n",
            "Iteration 384, loss = 1.55278323\n",
            "Iteration 385, loss = 1.43915655\n",
            "Iteration 386, loss = 1.42188303\n",
            "Iteration 387, loss = 1.42879888\n",
            "Iteration 388, loss = 1.44614145\n",
            "Iteration 389, loss = 1.47833016\n",
            "Iteration 390, loss = 1.41130557\n",
            "Iteration 391, loss = 1.39941423\n",
            "Iteration 392, loss = 1.42179332\n",
            "Iteration 393, loss = 1.48465034\n",
            "Iteration 394, loss = 1.42772368\n",
            "Iteration 395, loss = 1.42543558\n",
            "Iteration 396, loss = 1.42123963\n",
            "Iteration 397, loss = 1.39762016\n",
            "Iteration 398, loss = 1.38806798\n",
            "Iteration 399, loss = 1.41728567\n",
            "Iteration 400, loss = 1.41096228\n",
            "Iteration 401, loss = 1.36396522\n",
            "Iteration 402, loss = 1.35035683\n",
            "Iteration 403, loss = 1.36517558\n",
            "Iteration 404, loss = 1.36791450\n",
            "Iteration 405, loss = 1.41724667\n",
            "Iteration 406, loss = 1.45492674\n",
            "Iteration 407, loss = 1.31933840\n",
            "Iteration 408, loss = 1.35307988\n",
            "Iteration 409, loss = 1.38812117\n",
            "Iteration 410, loss = 1.32019069\n",
            "Iteration 411, loss = 1.31890469\n",
            "Iteration 412, loss = 1.36402781\n",
            "Iteration 413, loss = 1.29779901\n",
            "Iteration 414, loss = 1.31747642\n",
            "Iteration 415, loss = 1.32318373\n",
            "Iteration 416, loss = 1.29009279\n",
            "Iteration 417, loss = 1.38260511\n",
            "Iteration 418, loss = 1.28844039\n",
            "Iteration 419, loss = 1.33512356\n",
            "Iteration 420, loss = 1.32986143\n",
            "Iteration 421, loss = 1.29080625\n",
            "Iteration 422, loss = 1.30267760\n",
            "Iteration 423, loss = 1.27320257\n",
            "Iteration 424, loss = 1.24219756\n",
            "Iteration 425, loss = 1.28387825\n",
            "Iteration 426, loss = 1.26083926\n",
            "Iteration 427, loss = 1.27291478\n",
            "Iteration 428, loss = 1.33859022\n",
            "Iteration 429, loss = 1.24791140\n",
            "Iteration 430, loss = 1.30470818\n",
            "Iteration 431, loss = 1.27779261\n",
            "Iteration 432, loss = 1.22826382\n",
            "Iteration 433, loss = 1.28805902\n",
            "Iteration 434, loss = 1.27743700\n",
            "Iteration 435, loss = 1.22877886\n",
            "Iteration 436, loss = 1.20733015\n",
            "Iteration 437, loss = 1.27499498\n",
            "Iteration 438, loss = 1.25589664\n",
            "Iteration 439, loss = 1.23115595\n",
            "Iteration 440, loss = 1.17800669\n",
            "Iteration 441, loss = 1.22851795\n",
            "Iteration 442, loss = 1.19675962\n",
            "Iteration 443, loss = 1.16855634\n",
            "Iteration 444, loss = 1.19727371\n",
            "Iteration 445, loss = 1.20009755\n",
            "Iteration 446, loss = 1.27101445\n",
            "Iteration 447, loss = 1.18271010\n",
            "Iteration 448, loss = 1.16611485\n",
            "Iteration 449, loss = 1.23020289\n",
            "Iteration 450, loss = 1.15775185\n",
            "Iteration 451, loss = 1.18517410\n",
            "Iteration 452, loss = 1.13469028\n",
            "Iteration 453, loss = 1.15958442\n",
            "Iteration 454, loss = 1.16267762\n",
            "Iteration 455, loss = 1.15401621\n",
            "Iteration 456, loss = 1.16898976\n",
            "Iteration 457, loss = 1.14498298\n",
            "Iteration 458, loss = 1.15022791\n",
            "Iteration 459, loss = 1.12652791\n",
            "Iteration 460, loss = 1.10921165\n",
            "Iteration 461, loss = 1.13183338\n",
            "Iteration 462, loss = 1.16451790\n",
            "Iteration 463, loss = 1.13635283\n",
            "Iteration 464, loss = 1.13287735\n",
            "Iteration 465, loss = 1.12987472\n",
            "Iteration 466, loss = 1.10543552\n",
            "Iteration 467, loss = 1.15154783\n",
            "Iteration 468, loss = 1.09662179\n",
            "Iteration 469, loss = 1.23046435\n",
            "Iteration 470, loss = 1.14378200\n",
            "Iteration 471, loss = 1.14466675\n",
            "Iteration 472, loss = 1.03968341\n",
            "Iteration 473, loss = 1.07980622\n",
            "Iteration 474, loss = 1.03809626\n",
            "Iteration 475, loss = 1.11150401\n",
            "Iteration 476, loss = 1.04336062\n",
            "Iteration 477, loss = 1.13826076\n",
            "Iteration 478, loss = 1.07554925\n",
            "Iteration 479, loss = 1.07506379\n",
            "Iteration 480, loss = 1.07687181\n",
            "Iteration 481, loss = 1.05448431\n",
            "Iteration 482, loss = 1.04326823\n",
            "Iteration 483, loss = 1.03910027\n",
            "Iteration 484, loss = 0.99752735\n",
            "Iteration 485, loss = 1.14761440\n",
            "Iteration 486, loss = 1.11293049\n",
            "Iteration 487, loss = 1.05438923\n",
            "Iteration 488, loss = 1.08252947\n",
            "Iteration 489, loss = 0.98268366\n",
            "Iteration 490, loss = 1.03002333\n",
            "Iteration 491, loss = 0.98494085\n",
            "Iteration 492, loss = 1.06434968\n",
            "Iteration 493, loss = 1.01560541\n",
            "Iteration 494, loss = 1.00524383\n",
            "Iteration 495, loss = 0.97765240\n",
            "Iteration 496, loss = 1.03952505\n",
            "Iteration 497, loss = 1.02682009\n",
            "Iteration 498, loss = 0.98615908\n",
            "Iteration 499, loss = 1.03595226\n",
            "Iteration 500, loss = 1.00727582\n",
            "Iteration 501, loss = 1.01766123\n",
            "Iteration 502, loss = 0.93129580\n",
            "Iteration 503, loss = 0.96916509\n",
            "Iteration 504, loss = 1.00370390\n",
            "Iteration 505, loss = 0.94995849\n",
            "Iteration 506, loss = 0.95071205\n",
            "Iteration 507, loss = 0.98242643\n",
            "Iteration 508, loss = 0.94531971\n",
            "Iteration 509, loss = 0.95084619\n",
            "Iteration 510, loss = 0.96691138\n",
            "Iteration 511, loss = 1.00172719\n",
            "Iteration 512, loss = 0.92856325\n",
            "Iteration 513, loss = 0.91926051\n",
            "Iteration 514, loss = 0.89765502\n",
            "Iteration 515, loss = 0.93568063\n",
            "Iteration 516, loss = 0.97877748\n",
            "Iteration 517, loss = 0.88996695\n",
            "Iteration 518, loss = 0.90920194\n",
            "Iteration 519, loss = 0.89301836\n",
            "Iteration 520, loss = 0.89877202\n",
            "Iteration 521, loss = 0.92249004\n",
            "Iteration 522, loss = 0.92952723\n",
            "Iteration 523, loss = 0.88210846\n",
            "Iteration 524, loss = 0.92147655\n",
            "Iteration 525, loss = 0.90880956\n",
            "Iteration 526, loss = 0.93578242\n",
            "Iteration 527, loss = 0.89226178\n",
            "Iteration 528, loss = 0.87424948\n",
            "Iteration 529, loss = 0.87666226\n",
            "Iteration 530, loss = 0.87684191\n",
            "Iteration 531, loss = 0.90144370\n",
            "Iteration 532, loss = 1.01560872\n",
            "Iteration 533, loss = 0.88570800\n",
            "Iteration 534, loss = 0.82667112\n",
            "Iteration 535, loss = 0.87774919\n",
            "Iteration 536, loss = 0.89621768\n",
            "Iteration 537, loss = 0.89462627\n",
            "Iteration 538, loss = 0.92185883\n",
            "Iteration 539, loss = 0.85963224\n",
            "Iteration 540, loss = 0.84466723\n",
            "Iteration 541, loss = 0.94957936\n",
            "Iteration 542, loss = 0.89255282\n",
            "Iteration 543, loss = 0.86346579\n",
            "Iteration 544, loss = 0.87647088\n",
            "Iteration 545, loss = 0.80661468\n",
            "Iteration 546, loss = 0.84498075\n",
            "Iteration 547, loss = 0.85434023\n",
            "Iteration 548, loss = 0.84191064\n",
            "Iteration 549, loss = 0.85102051\n",
            "Iteration 550, loss = 0.82553484\n",
            "Iteration 551, loss = 0.82699544\n",
            "Iteration 552, loss = 0.79762712\n",
            "Iteration 553, loss = 0.83867537\n",
            "Iteration 554, loss = 0.83777029\n",
            "Iteration 555, loss = 0.88039343\n",
            "Iteration 556, loss = 0.79191488\n",
            "Iteration 557, loss = 0.82099741\n",
            "Iteration 558, loss = 0.77842642\n",
            "Iteration 559, loss = 0.85428585\n",
            "Iteration 560, loss = 0.88032622\n",
            "Iteration 561, loss = 0.84904156\n",
            "Iteration 562, loss = 0.75652238\n",
            "Iteration 563, loss = 0.77823538\n",
            "Iteration 564, loss = 0.83411222\n",
            "Iteration 565, loss = 0.75336282\n",
            "Iteration 566, loss = 0.78005315\n",
            "Iteration 567, loss = 0.77598234\n",
            "Iteration 568, loss = 0.77932747\n",
            "Iteration 569, loss = 0.81196092\n",
            "Iteration 570, loss = 0.81590597\n",
            "Iteration 571, loss = 0.75955392\n",
            "Iteration 572, loss = 0.78390458\n",
            "Iteration 573, loss = 0.75450458\n",
            "Iteration 574, loss = 0.75405966\n",
            "Iteration 575, loss = 0.71933416\n",
            "Iteration 576, loss = 0.79572417\n",
            "Iteration 577, loss = 0.78996017\n",
            "Iteration 578, loss = 0.76214858\n",
            "Iteration 579, loss = 0.75939747\n",
            "Iteration 580, loss = 0.87066049\n",
            "Iteration 581, loss = 0.73793770\n",
            "Iteration 582, loss = 0.75950924\n",
            "Iteration 583, loss = 0.78233856\n",
            "Iteration 584, loss = 0.70724864\n",
            "Iteration 585, loss = 0.72792569\n",
            "Iteration 586, loss = 0.69651282\n",
            "Iteration 587, loss = 0.73785689\n",
            "Iteration 588, loss = 0.81937502\n",
            "Iteration 589, loss = 0.70692392\n",
            "Iteration 590, loss = 0.81118737\n",
            "Iteration 591, loss = 0.76951442\n",
            "Iteration 592, loss = 0.69160964\n",
            "Iteration 593, loss = 0.80417081\n",
            "Iteration 594, loss = 0.76318490\n",
            "Iteration 595, loss = 0.66480198\n",
            "Iteration 596, loss = 0.74136044\n",
            "Iteration 597, loss = 0.71138371\n",
            "Iteration 598, loss = 0.79474503\n",
            "Iteration 599, loss = 0.71298922\n",
            "Iteration 600, loss = 0.65947867\n",
            "Iteration 601, loss = 0.66946779\n",
            "Iteration 602, loss = 0.68330395\n",
            "Iteration 603, loss = 0.66526314\n",
            "Iteration 604, loss = 0.71169225\n",
            "Iteration 605, loss = 0.68592250\n",
            "Iteration 606, loss = 0.74964088\n",
            "Iteration 607, loss = 0.68459666\n",
            "Iteration 608, loss = 0.68903746\n",
            "Iteration 609, loss = 0.66905914\n",
            "Iteration 610, loss = 0.67621625\n",
            "Iteration 611, loss = 0.71325743\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 612, loss = 0.68217443\n",
            "Iteration 613, loss = 0.58619844\n",
            "Iteration 614, loss = 0.56705008\n",
            "Iteration 615, loss = 0.55912867\n",
            "Iteration 616, loss = 0.55730178\n",
            "Iteration 617, loss = 0.55644976\n",
            "Iteration 618, loss = 0.55766644\n",
            "Iteration 619, loss = 0.55582496\n",
            "Iteration 620, loss = 0.55532950\n",
            "Iteration 621, loss = 0.55477371\n",
            "Iteration 622, loss = 0.55268947\n",
            "Iteration 623, loss = 0.55382738\n",
            "Iteration 624, loss = 0.55320600\n",
            "Iteration 625, loss = 0.55198432\n",
            "Iteration 626, loss = 0.55063091\n",
            "Iteration 627, loss = 0.54982020\n",
            "Iteration 628, loss = 0.54803711\n",
            "Iteration 629, loss = 0.54952240\n",
            "Iteration 630, loss = 0.55135331\n",
            "Iteration 631, loss = 0.55006349\n",
            "Iteration 632, loss = 0.54975434\n",
            "Iteration 633, loss = 0.54655621\n",
            "Iteration 634, loss = 0.54898176\n",
            "Iteration 635, loss = 0.54835839\n",
            "Iteration 636, loss = 0.54567031\n",
            "Iteration 637, loss = 0.54586729\n",
            "Iteration 638, loss = 0.54585590\n",
            "Iteration 639, loss = 0.54557360\n",
            "Iteration 640, loss = 0.54256997\n",
            "Iteration 641, loss = 0.54359366\n",
            "Iteration 642, loss = 0.54262676\n",
            "Iteration 643, loss = 0.54332228\n",
            "Iteration 644, loss = 0.54351729\n",
            "Iteration 645, loss = 0.54102803\n",
            "Iteration 646, loss = 0.54129041\n",
            "Iteration 647, loss = 0.54109700\n",
            "Iteration 648, loss = 0.54149137\n",
            "Iteration 649, loss = 0.54015985\n",
            "Iteration 650, loss = 0.53666876\n",
            "Iteration 651, loss = 0.53968327\n",
            "Iteration 652, loss = 0.53749002\n",
            "Iteration 653, loss = 0.53697161\n",
            "Iteration 654, loss = 0.53565410\n",
            "Iteration 655, loss = 0.53745405\n",
            "Iteration 656, loss = 0.53673771\n",
            "Iteration 657, loss = 0.53359211\n",
            "Iteration 658, loss = 0.53520236\n",
            "Iteration 659, loss = 0.53431518\n",
            "Iteration 660, loss = 0.53460834\n",
            "Iteration 661, loss = 0.53358867\n",
            "Iteration 662, loss = 0.53391067\n",
            "Iteration 663, loss = 0.53113321\n",
            "Iteration 664, loss = 0.53263168\n",
            "Iteration 665, loss = 0.53059104\n",
            "Iteration 666, loss = 0.52999437\n",
            "Iteration 667, loss = 0.53011941\n",
            "Iteration 668, loss = 0.52891850\n",
            "Iteration 669, loss = 0.53002737\n",
            "Iteration 670, loss = 0.52939887\n",
            "Iteration 671, loss = 0.52921110\n",
            "Iteration 672, loss = 0.52755114\n",
            "Iteration 673, loss = 0.52625436\n",
            "Iteration 674, loss = 0.52562802\n",
            "Iteration 675, loss = 0.52729826\n",
            "Iteration 676, loss = 0.52552739\n",
            "Iteration 677, loss = 0.52606833\n",
            "Iteration 678, loss = 0.52746570\n",
            "Iteration 679, loss = 0.52628620\n",
            "Iteration 680, loss = 0.52507020\n",
            "Iteration 681, loss = 0.52354696\n",
            "Iteration 682, loss = 0.52288255\n",
            "Iteration 683, loss = 0.52277695\n",
            "Iteration 684, loss = 0.52437990\n",
            "Iteration 685, loss = 0.52222028\n",
            "Iteration 686, loss = 0.52352867\n",
            "Iteration 687, loss = 0.52227917\n",
            "Iteration 688, loss = 0.52065234\n",
            "Iteration 689, loss = 0.51963989\n",
            "Iteration 690, loss = 0.52001311\n",
            "Iteration 691, loss = 0.51956580\n",
            "Iteration 692, loss = 0.51933853\n",
            "Iteration 693, loss = 0.51953495\n",
            "Iteration 694, loss = 0.51819490\n",
            "Iteration 695, loss = 0.51903800\n",
            "Iteration 696, loss = 0.51776320\n",
            "Iteration 697, loss = 0.51662017\n",
            "Iteration 698, loss = 0.51867809\n",
            "Iteration 699, loss = 0.51741683\n",
            "Iteration 700, loss = 0.51593028\n",
            "Iteration 701, loss = 0.51708654\n",
            "Iteration 702, loss = 0.51540900\n",
            "Iteration 703, loss = 0.51456698\n",
            "Iteration 704, loss = 0.51456220\n",
            "Iteration 705, loss = 0.51546666\n",
            "Iteration 706, loss = 0.51574034\n",
            "Iteration 707, loss = 0.51251028\n",
            "Iteration 708, loss = 0.51274642\n",
            "Iteration 709, loss = 0.51185156\n",
            "Iteration 710, loss = 0.51227489\n",
            "Iteration 711, loss = 0.51185104\n",
            "Iteration 712, loss = 0.51201672\n",
            "Iteration 713, loss = 0.51109916\n",
            "Iteration 714, loss = 0.51071360\n",
            "Iteration 715, loss = 0.51158358\n",
            "Iteration 716, loss = 0.51050165\n",
            "Iteration 717, loss = 0.51155468\n",
            "Iteration 718, loss = 0.50961662\n",
            "Iteration 719, loss = 0.50900700\n",
            "Iteration 720, loss = 0.51006421\n",
            "Iteration 721, loss = 0.51020940\n",
            "Iteration 722, loss = 0.50799383\n",
            "Iteration 723, loss = 0.50677125\n",
            "Iteration 724, loss = 0.50712543\n",
            "Iteration 725, loss = 0.50740926\n",
            "Iteration 726, loss = 0.50833291\n",
            "Iteration 727, loss = 0.50561070\n",
            "Iteration 728, loss = 0.50413540\n",
            "Iteration 729, loss = 0.50558561\n",
            "Iteration 730, loss = 0.50468722\n",
            "Iteration 731, loss = 0.50382499\n",
            "Iteration 732, loss = 0.50316137\n",
            "Iteration 733, loss = 0.50565682\n",
            "Iteration 734, loss = 0.50318499\n",
            "Iteration 735, loss = 0.50350536\n",
            "Iteration 736, loss = 0.50315671\n",
            "Iteration 737, loss = 0.50269714\n",
            "Iteration 738, loss = 0.50194972\n",
            "Iteration 739, loss = 0.50102823\n",
            "Iteration 740, loss = 0.50258715\n",
            "Iteration 741, loss = 0.50142642\n",
            "Iteration 742, loss = 0.50081217\n",
            "Iteration 743, loss = 0.50063378\n",
            "Iteration 744, loss = 0.49969115\n",
            "Iteration 745, loss = 0.50093688\n",
            "Iteration 746, loss = 0.50053040\n",
            "Iteration 747, loss = 0.50004370\n",
            "Iteration 748, loss = 0.49913302\n",
            "Iteration 749, loss = 0.49810230\n",
            "Iteration 750, loss = 0.49744124\n",
            "Iteration 751, loss = 0.49852005\n",
            "Iteration 752, loss = 0.49857347\n",
            "Iteration 753, loss = 0.49705624\n",
            "Iteration 754, loss = 0.49711316\n",
            "Iteration 755, loss = 0.49694092\n",
            "Iteration 756, loss = 0.49611551\n",
            "Iteration 757, loss = 0.49511724\n",
            "Iteration 758, loss = 0.49618465\n",
            "Iteration 759, loss = 0.49363864\n",
            "Iteration 760, loss = 0.49562974\n",
            "Iteration 761, loss = 0.49278206\n",
            "Iteration 762, loss = 0.49353864\n",
            "Iteration 763, loss = 0.49085679\n",
            "Iteration 764, loss = 0.49289559\n",
            "Iteration 765, loss = 0.49111595\n",
            "Iteration 766, loss = 0.49191485\n",
            "Iteration 767, loss = 0.49417323\n",
            "Iteration 768, loss = 0.49193127\n",
            "Iteration 769, loss = 0.49153343\n",
            "Iteration 770, loss = 0.49005211\n",
            "Iteration 771, loss = 0.49111476\n",
            "Iteration 772, loss = 0.48889560\n",
            "Iteration 773, loss = 0.48813492\n",
            "Iteration 774, loss = 0.49043605\n",
            "Iteration 775, loss = 0.48895634\n",
            "Iteration 776, loss = 0.48965365\n",
            "Iteration 777, loss = 0.48708363\n",
            "Iteration 778, loss = 0.48739846\n",
            "Iteration 779, loss = 0.48669921\n",
            "Iteration 780, loss = 0.48699813\n",
            "Iteration 781, loss = 0.48864111\n",
            "Iteration 782, loss = 0.48797577\n",
            "Iteration 783, loss = 0.48541037\n",
            "Iteration 784, loss = 0.48563499\n",
            "Iteration 785, loss = 0.48567012\n",
            "Iteration 786, loss = 0.48389629\n",
            "Iteration 787, loss = 0.48306318\n",
            "Iteration 788, loss = 0.48385763\n",
            "Iteration 789, loss = 0.48368665\n",
            "Iteration 790, loss = 0.48355905\n",
            "Iteration 791, loss = 0.48399740\n",
            "Iteration 792, loss = 0.48288910\n",
            "Iteration 793, loss = 0.48188115\n",
            "Iteration 794, loss = 0.48119631\n",
            "Iteration 795, loss = 0.48206346\n",
            "Iteration 796, loss = 0.48125774\n",
            "Iteration 797, loss = 0.48170934\n",
            "Iteration 798, loss = 0.47995630\n",
            "Iteration 799, loss = 0.47927908\n",
            "Iteration 800, loss = 0.47848228\n",
            "Iteration 801, loss = 0.47954952\n",
            "Iteration 802, loss = 0.47912878\n",
            "Iteration 803, loss = 0.47883651\n",
            "Iteration 804, loss = 0.47889617\n",
            "Iteration 805, loss = 0.47810449\n",
            "Iteration 806, loss = 0.47799112\n",
            "Iteration 807, loss = 0.47740385\n",
            "Iteration 808, loss = 0.47770831\n",
            "Iteration 809, loss = 0.47717501\n",
            "Iteration 810, loss = 0.47833569\n",
            "Iteration 811, loss = 0.47586830\n",
            "Iteration 812, loss = 0.47473786\n",
            "Iteration 813, loss = 0.47401552\n",
            "Iteration 814, loss = 0.47518355\n",
            "Iteration 815, loss = 0.47468514\n",
            "Iteration 816, loss = 0.47505732\n",
            "Iteration 817, loss = 0.47426686\n",
            "Iteration 818, loss = 0.47327964\n",
            "Iteration 819, loss = 0.47545010\n",
            "Iteration 820, loss = 0.47346159\n",
            "Iteration 821, loss = 0.47437650\n",
            "Iteration 822, loss = 0.47318761\n",
            "Iteration 823, loss = 0.47127269\n",
            "Iteration 824, loss = 0.47234731\n",
            "Iteration 825, loss = 0.47302509\n",
            "Iteration 826, loss = 0.47219542\n",
            "Iteration 827, loss = 0.47139296\n",
            "Iteration 828, loss = 0.46980104\n",
            "Iteration 829, loss = 0.47202689\n",
            "Iteration 830, loss = 0.46822401\n",
            "Iteration 831, loss = 0.46870911\n",
            "Iteration 832, loss = 0.46845614\n",
            "Iteration 833, loss = 0.46912840\n",
            "Iteration 834, loss = 0.46943237\n",
            "Iteration 835, loss = 0.47024071\n",
            "Iteration 836, loss = 0.46928203\n",
            "Iteration 837, loss = 0.46855751\n",
            "Iteration 838, loss = 0.46761091\n",
            "Iteration 839, loss = 0.46686304\n",
            "Iteration 840, loss = 0.46875320\n",
            "Iteration 841, loss = 0.46429736\n",
            "Iteration 842, loss = 0.46623365\n",
            "Iteration 843, loss = 0.46666840\n",
            "Iteration 844, loss = 0.46645964\n",
            "Iteration 845, loss = 0.46441107\n",
            "Iteration 846, loss = 0.46597868\n",
            "Iteration 847, loss = 0.46422827\n",
            "Iteration 848, loss = 0.46347032\n",
            "Iteration 849, loss = 0.46404733\n",
            "Iteration 850, loss = 0.46366780\n",
            "Iteration 851, loss = 0.46430431\n",
            "Iteration 852, loss = 0.46336859\n",
            "Iteration 853, loss = 0.46295482\n",
            "Iteration 854, loss = 0.46400524\n",
            "Iteration 855, loss = 0.46214652\n",
            "Iteration 856, loss = 0.46111391\n",
            "Iteration 857, loss = 0.46177572\n",
            "Iteration 858, loss = 0.46112721\n",
            "Iteration 859, loss = 0.46048091\n",
            "Iteration 860, loss = 0.45942109\n",
            "Iteration 861, loss = 0.45966459\n",
            "Iteration 862, loss = 0.45979800\n",
            "Iteration 863, loss = 0.45988992\n",
            "Iteration 864, loss = 0.45991854\n",
            "Iteration 865, loss = 0.45871435\n",
            "Iteration 866, loss = 0.45821849\n",
            "Iteration 867, loss = 0.45790004\n",
            "Iteration 868, loss = 0.45722657\n",
            "Iteration 869, loss = 0.45960295\n",
            "Iteration 870, loss = 0.45696659\n",
            "Iteration 871, loss = 0.45627871\n",
            "Iteration 872, loss = 0.45659788\n",
            "Iteration 873, loss = 0.45712570\n",
            "Iteration 874, loss = 0.45604493\n",
            "Iteration 875, loss = 0.45602107\n",
            "Iteration 876, loss = 0.45564950\n",
            "Iteration 877, loss = 0.45477802\n",
            "Iteration 878, loss = 0.45538744\n",
            "Iteration 879, loss = 0.45555822\n",
            "Iteration 880, loss = 0.45367475\n",
            "Iteration 881, loss = 0.45511121\n",
            "Iteration 882, loss = 0.45405136\n",
            "Iteration 883, loss = 0.45390321\n",
            "Iteration 884, loss = 0.45296609\n",
            "Iteration 885, loss = 0.45203885\n",
            "Iteration 886, loss = 0.45178110\n",
            "Iteration 887, loss = 0.45061218\n",
            "Iteration 888, loss = 0.45174595\n",
            "Iteration 889, loss = 0.45286715\n",
            "Iteration 890, loss = 0.45261052\n",
            "Iteration 891, loss = 0.45132972\n",
            "Iteration 892, loss = 0.44978004\n",
            "Iteration 893, loss = 0.45165993\n",
            "Iteration 894, loss = 0.45018899\n",
            "Iteration 895, loss = 0.44930289\n",
            "Iteration 896, loss = 0.44869593\n",
            "Iteration 897, loss = 0.44719236\n",
            "Iteration 898, loss = 0.44852642\n",
            "Iteration 899, loss = 0.44751632\n",
            "Iteration 900, loss = 0.44783393\n",
            "Iteration 901, loss = 0.44784341\n",
            "Iteration 902, loss = 0.44664450\n",
            "Iteration 903, loss = 0.44628614\n",
            "Iteration 904, loss = 0.44706767\n",
            "Iteration 905, loss = 0.44562486\n",
            "Iteration 906, loss = 0.44660885\n",
            "Iteration 907, loss = 0.44610090\n",
            "Iteration 908, loss = 0.44465372\n",
            "Iteration 909, loss = 0.44480152\n",
            "Iteration 910, loss = 0.44446810\n",
            "Iteration 911, loss = 0.44375703\n",
            "Iteration 912, loss = 0.44415030\n",
            "Iteration 913, loss = 0.44322595\n",
            "Iteration 914, loss = 0.44383584\n",
            "Iteration 915, loss = 0.44447685\n",
            "Iteration 916, loss = 0.44307132\n",
            "Iteration 917, loss = 0.44162896\n",
            "Iteration 918, loss = 0.44188202\n",
            "Iteration 919, loss = 0.44307288\n",
            "Iteration 920, loss = 0.44244184\n",
            "Iteration 921, loss = 0.44385343\n",
            "Iteration 922, loss = 0.44160432\n",
            "Iteration 923, loss = 0.43980088\n",
            "Iteration 924, loss = 0.44061885\n",
            "Iteration 925, loss = 0.44023579\n",
            "Iteration 926, loss = 0.44237658\n",
            "Iteration 927, loss = 0.44011579\n",
            "Iteration 928, loss = 0.44080088\n",
            "Iteration 929, loss = 0.43797287\n",
            "Iteration 930, loss = 0.43850197\n",
            "Iteration 931, loss = 0.44004037\n",
            "Iteration 932, loss = 0.43844952\n",
            "Iteration 933, loss = 0.43856628\n",
            "Iteration 934, loss = 0.43727468\n",
            "Iteration 935, loss = 0.43874538\n",
            "Iteration 936, loss = 0.43745202\n",
            "Iteration 937, loss = 0.43640773\n",
            "Iteration 938, loss = 0.43671048\n",
            "Iteration 939, loss = 0.43630746\n",
            "Iteration 940, loss = 0.43622402\n",
            "Iteration 941, loss = 0.43590846\n",
            "Iteration 942, loss = 0.43672560\n",
            "Iteration 943, loss = 0.43542755\n",
            "Iteration 944, loss = 0.43518981\n",
            "Iteration 945, loss = 0.43470543\n",
            "Iteration 946, loss = 0.43501111\n",
            "Iteration 947, loss = 0.43277186\n",
            "Iteration 948, loss = 0.43224249\n",
            "Iteration 949, loss = 0.43457315\n",
            "Iteration 950, loss = 0.43373760\n",
            "Iteration 951, loss = 0.43382751\n",
            "Iteration 952, loss = 0.43220275\n",
            "Iteration 953, loss = 0.43284917\n",
            "Iteration 954, loss = 0.43210340\n",
            "Iteration 955, loss = 0.43218650\n",
            "Iteration 956, loss = 0.43243710\n",
            "Iteration 957, loss = 0.43123753\n",
            "Iteration 958, loss = 0.43082698\n",
            "Iteration 959, loss = 0.43057666\n",
            "Iteration 960, loss = 0.43071016\n",
            "Iteration 961, loss = 0.42957923\n",
            "Iteration 962, loss = 0.43019697\n",
            "Iteration 963, loss = 0.42927547\n",
            "Iteration 964, loss = 0.42998722\n",
            "Iteration 965, loss = 0.43161021\n",
            "Iteration 966, loss = 0.43090515\n",
            "Iteration 967, loss = 0.42925453\n",
            "Iteration 968, loss = 0.42832371\n",
            "Iteration 969, loss = 0.42844114\n",
            "Iteration 970, loss = 0.42830901\n",
            "Iteration 971, loss = 0.42853439\n",
            "Iteration 972, loss = 0.42782367\n",
            "Iteration 973, loss = 0.42781926\n",
            "Iteration 974, loss = 0.42623778\n",
            "Iteration 975, loss = 0.42640443\n",
            "Iteration 976, loss = 0.42518630\n",
            "Iteration 977, loss = 0.42685238\n",
            "Iteration 978, loss = 0.42579823\n",
            "Iteration 979, loss = 0.42671046\n",
            "Iteration 980, loss = 0.42529678\n",
            "Iteration 981, loss = 0.42439538\n",
            "Iteration 982, loss = 0.42404599\n",
            "Iteration 983, loss = 0.42518739\n",
            "Iteration 984, loss = 0.42388584\n",
            "Iteration 985, loss = 0.42249688\n",
            "Iteration 986, loss = 0.42276973\n",
            "Iteration 987, loss = 0.42373897\n",
            "Iteration 988, loss = 0.42340875\n",
            "Iteration 989, loss = 0.42269282\n",
            "Iteration 990, loss = 0.42227814\n",
            "Iteration 991, loss = 0.42309065\n",
            "Iteration 992, loss = 0.42314924\n",
            "Iteration 993, loss = 0.42116163\n",
            "Iteration 994, loss = 0.42019356\n",
            "Iteration 995, loss = 0.42224397\n",
            "Iteration 996, loss = 0.42045308\n",
            "Iteration 997, loss = 0.42142701\n",
            "Iteration 998, loss = 0.42039311\n",
            "Iteration 999, loss = 0.41954173\n",
            "Iteration 1000, loss = 0.42033616\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=1536, learning_rate='adaptive', max_iter=1000,\n",
              "              solver='sgd', verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "plot(classifier.loss_curve_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9IChOBsGSFwK",
        "outputId": "a1560fe4-1d70-40e4-e83f-2718c0287b04"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f19e18d6c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Use the trained model to generate text"
      ],
      "metadata": {
        "id": "aSGTsbN4Tj6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_token = lambda text: str(output_tokeniser.decode(classifier.predict([input_tokeniser.encode(text).vector])))"
      ],
      "metadata": {
        "id": "7M6bhVTl9MKb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_token(\"the\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q6Gj-NQm-xUb",
        "outputId": "4c2386ab-43c0-4abf-ffae-d4a22c38c8c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'name'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt:str) -> str:\n",
        "  sequence = prompt\n",
        "  for _ in range(10):\n",
        "    sequence += f\" {predict_next_token(sequence)}\"\n",
        "  return sequence"
      ],
      "metadata": {
        "id": "cFyM-GiM99H6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"God\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YRaaGpqs9uAV",
        "outputId": "308a68ef-beba-41d2-fffc-5e6d7cb89925"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'God  unknown  the  unknown   unknown  beast  unknown  ground ground for for'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Save the trained model"
      ],
      "metadata": {
        "id": "Oatz_hkMTofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "PATH = \"language_model.joblib.pkl\"\n",
        "dump(classifier, PATH, compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FOkQ2L9Sb_I",
        "outputId": "aa6f6ba3-7610-4501-9439-56277d237d4d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language_model.joblib.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}