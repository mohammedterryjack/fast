{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFast for NLG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Get some training data"
      ],
      "metadata": {
        "id": "mFMJBV2nStUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk import download\n",
        "download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJovm_SzJ46u",
        "outputId": "89a9bc6c-34b7-41f2-ac83-d90e7602a269"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(brown.sents())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF2w52dlJ6xA",
        "outputId": "22191e58-3f76-44aa-e0d3-de59e82d232b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "  for tokens in brown.sents()[:100]:\n",
        "    for index in range(1,len(tokens)):\n",
        "      yield tokens[:index],tokens[index]"
      ],
      "metadata": {
        "id": "7TrDwNT67Hwp"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = zip(*prepare_data())"
      ],
      "metadata": {
        "id": "VT7q3gyh8t28"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "634F2irDMuuP",
        "outputId": "6cbe57c5-d430-4f9b-89a3-54796a006146"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "560YuLtGM74z",
        "outputId": "1239f540-4840-486b-f9d4-eec84b9eadc2"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Jury'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Encode the data using FFast"
      ],
      "metadata": {
        "id": "oJjPpOVvSyGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Y5fR1P5hqf",
        "outputId": "23277d94-1625-40ff-cd7a-32889816100e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffast in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ffast) (3.2.5)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (from ffast) (1.3.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.4.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from ffast) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ffast) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ffast) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ffast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ffast import load"
      ],
      "metadata": {
        "id": "d1MK1Q0z6GgI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser = load(\"poincare\")"
      ],
      "metadata": {
        "id": "b8f-C_4M6LKh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = list(map(lambda tokens:tokeniser.encode(' '.join(tokens)).vector,x))\n",
        "y_train = list(map(lambda token:tokeniser.encode(token).ids[0], y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEOTUHdXMS-W",
        "outputId": "5a335f07-fc74-4704-9b46-400bf8cbc7e2"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Tran a model"
      ],
      "metadata": {
        "id": "n7NVHG0FS68v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "classifier = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,100,100,100,100), \n",
        "    solver='sgd',\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=1000,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZylJfmQe8wzg"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L5JZtlY9EQ-",
        "outputId": "2269ad71-f600-4ef8-a296-c6f01d204707"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 6.69838307\n",
            "Iteration 2, loss = 6.44641106\n",
            "Iteration 3, loss = 6.25441602\n",
            "Iteration 4, loss = 6.11376073\n",
            "Iteration 5, loss = 5.99208247\n",
            "Iteration 6, loss = 5.88514483\n",
            "Iteration 7, loss = 5.80194387\n",
            "Iteration 8, loss = 5.73731535\n",
            "Iteration 9, loss = 5.68580632\n",
            "Iteration 10, loss = 5.64352334\n",
            "Iteration 11, loss = 5.60996077\n",
            "Iteration 12, loss = 5.58421566\n",
            "Iteration 13, loss = 5.55933605\n",
            "Iteration 14, loss = 5.53968250\n",
            "Iteration 15, loss = 5.52318617\n",
            "Iteration 16, loss = 5.51001203\n",
            "Iteration 17, loss = 5.49646448\n",
            "Iteration 18, loss = 5.48449484\n",
            "Iteration 19, loss = 5.47385182\n",
            "Iteration 20, loss = 5.46664444\n",
            "Iteration 21, loss = 5.45693058\n",
            "Iteration 22, loss = 5.45146670\n",
            "Iteration 23, loss = 5.44054439\n",
            "Iteration 24, loss = 5.43437521\n",
            "Iteration 25, loss = 5.42846690\n",
            "Iteration 26, loss = 5.41912540\n",
            "Iteration 27, loss = 5.41328942\n",
            "Iteration 28, loss = 5.40817712\n",
            "Iteration 29, loss = 5.40321155\n",
            "Iteration 30, loss = 5.39775449\n",
            "Iteration 31, loss = 5.39093516\n",
            "Iteration 32, loss = 5.38897505\n",
            "Iteration 33, loss = 5.38153601\n",
            "Iteration 34, loss = 5.37991775\n",
            "Iteration 35, loss = 5.37501833\n",
            "Iteration 36, loss = 5.37389132\n",
            "Iteration 37, loss = 5.37076533\n",
            "Iteration 38, loss = 5.36259626\n",
            "Iteration 39, loss = 5.35784072\n",
            "Iteration 40, loss = 5.35761572\n",
            "Iteration 41, loss = 5.35207944\n",
            "Iteration 42, loss = 5.34871348\n",
            "Iteration 43, loss = 5.34545353\n",
            "Iteration 44, loss = 5.34140513\n",
            "Iteration 45, loss = 5.34021950\n",
            "Iteration 46, loss = 5.33586261\n",
            "Iteration 47, loss = 5.33703850\n",
            "Iteration 48, loss = 5.32886744\n",
            "Iteration 49, loss = 5.32853788\n",
            "Iteration 50, loss = 5.32560008\n",
            "Iteration 51, loss = 5.31986090\n",
            "Iteration 52, loss = 5.31718114\n",
            "Iteration 53, loss = 5.31563234\n",
            "Iteration 54, loss = 5.31177523\n",
            "Iteration 55, loss = 5.31150417\n",
            "Iteration 56, loss = 5.30700424\n",
            "Iteration 57, loss = 5.30398868\n",
            "Iteration 58, loss = 5.30063813\n",
            "Iteration 59, loss = 5.29937731\n",
            "Iteration 60, loss = 5.29815864\n",
            "Iteration 61, loss = 5.29413516\n",
            "Iteration 62, loss = 5.29183820\n",
            "Iteration 63, loss = 5.29077744\n",
            "Iteration 64, loss = 5.28563513\n",
            "Iteration 65, loss = 5.28348526\n",
            "Iteration 66, loss = 5.28232380\n",
            "Iteration 67, loss = 5.27844941\n",
            "Iteration 68, loss = 5.27596789\n",
            "Iteration 69, loss = 5.27784522\n",
            "Iteration 70, loss = 5.27155594\n",
            "Iteration 71, loss = 5.27266803\n",
            "Iteration 72, loss = 5.26683018\n",
            "Iteration 73, loss = 5.26155272\n",
            "Iteration 74, loss = 5.26472902\n",
            "Iteration 75, loss = 5.25855871\n",
            "Iteration 76, loss = 5.26029074\n",
            "Iteration 77, loss = 5.25577255\n",
            "Iteration 78, loss = 5.25002628\n",
            "Iteration 79, loss = 5.24828073\n",
            "Iteration 80, loss = 5.24602305\n",
            "Iteration 81, loss = 5.24279146\n",
            "Iteration 82, loss = 5.23998330\n",
            "Iteration 83, loss = 5.23966927\n",
            "Iteration 84, loss = 5.23547872\n",
            "Iteration 85, loss = 5.23711703\n",
            "Iteration 86, loss = 5.23151670\n",
            "Iteration 87, loss = 5.22846573\n",
            "Iteration 88, loss = 5.22703290\n",
            "Iteration 89, loss = 5.22612483\n",
            "Iteration 90, loss = 5.22239631\n",
            "Iteration 91, loss = 5.21967506\n",
            "Iteration 92, loss = 5.21794738\n",
            "Iteration 93, loss = 5.21575448\n",
            "Iteration 94, loss = 5.21469481\n",
            "Iteration 95, loss = 5.21044316\n",
            "Iteration 96, loss = 5.20734105\n",
            "Iteration 97, loss = 5.20392841\n",
            "Iteration 98, loss = 5.20203006\n",
            "Iteration 99, loss = 5.20209355\n",
            "Iteration 100, loss = 5.19501741\n",
            "Iteration 101, loss = 5.19221206\n",
            "Iteration 102, loss = 5.19073223\n",
            "Iteration 103, loss = 5.18863268\n",
            "Iteration 104, loss = 5.18758473\n",
            "Iteration 105, loss = 5.18311559\n",
            "Iteration 106, loss = 5.17870527\n",
            "Iteration 107, loss = 5.17923096\n",
            "Iteration 108, loss = 5.17865352\n",
            "Iteration 109, loss = 5.17535373\n",
            "Iteration 110, loss = 5.17518433\n",
            "Iteration 111, loss = 5.16728726\n",
            "Iteration 112, loss = 5.16445440\n",
            "Iteration 113, loss = 5.16571886\n",
            "Iteration 114, loss = 5.15922298\n",
            "Iteration 115, loss = 5.15504987\n",
            "Iteration 116, loss = 5.15700150\n",
            "Iteration 117, loss = 5.15165667\n",
            "Iteration 118, loss = 5.14986182\n",
            "Iteration 119, loss = 5.15142086\n",
            "Iteration 120, loss = 5.13849884\n",
            "Iteration 121, loss = 5.14166540\n",
            "Iteration 122, loss = 5.13911805\n",
            "Iteration 123, loss = 5.13454883\n",
            "Iteration 124, loss = 5.13192005\n",
            "Iteration 125, loss = 5.12851553\n",
            "Iteration 126, loss = 5.12469581\n",
            "Iteration 127, loss = 5.12161501\n",
            "Iteration 128, loss = 5.12126943\n",
            "Iteration 129, loss = 5.11819203\n",
            "Iteration 130, loss = 5.11627056\n",
            "Iteration 131, loss = 5.10945163\n",
            "Iteration 132, loss = 5.10834680\n",
            "Iteration 133, loss = 5.10122450\n",
            "Iteration 134, loss = 5.10167743\n",
            "Iteration 135, loss = 5.10144190\n",
            "Iteration 136, loss = 5.09516971\n",
            "Iteration 137, loss = 5.09610600\n",
            "Iteration 138, loss = 5.09352347\n",
            "Iteration 139, loss = 5.09007828\n",
            "Iteration 140, loss = 5.08662356\n",
            "Iteration 141, loss = 5.07756357\n",
            "Iteration 142, loss = 5.08011515\n",
            "Iteration 143, loss = 5.07814417\n",
            "Iteration 144, loss = 5.07249236\n",
            "Iteration 145, loss = 5.07060168\n",
            "Iteration 146, loss = 5.06359337\n",
            "Iteration 147, loss = 5.05663857\n",
            "Iteration 148, loss = 5.06494944\n",
            "Iteration 149, loss = 5.05831501\n",
            "Iteration 150, loss = 5.05088571\n",
            "Iteration 151, loss = 5.04907937\n",
            "Iteration 152, loss = 5.04092175\n",
            "Iteration 153, loss = 5.03970933\n",
            "Iteration 154, loss = 5.04532430\n",
            "Iteration 155, loss = 5.02799222\n",
            "Iteration 156, loss = 5.03122564\n",
            "Iteration 157, loss = 5.02799909\n",
            "Iteration 158, loss = 5.02715726\n",
            "Iteration 159, loss = 5.01449686\n",
            "Iteration 160, loss = 5.02001197\n",
            "Iteration 161, loss = 5.00893361\n",
            "Iteration 162, loss = 5.00981553\n",
            "Iteration 163, loss = 5.01026325\n",
            "Iteration 164, loss = 5.00578611\n",
            "Iteration 165, loss = 5.00704962\n",
            "Iteration 166, loss = 4.98721163\n",
            "Iteration 167, loss = 4.98876659\n",
            "Iteration 168, loss = 4.98895805\n",
            "Iteration 169, loss = 4.98783390\n",
            "Iteration 170, loss = 4.97466253\n",
            "Iteration 171, loss = 4.98199319\n",
            "Iteration 172, loss = 4.97145865\n",
            "Iteration 173, loss = 4.96565814\n",
            "Iteration 174, loss = 4.95488687\n",
            "Iteration 175, loss = 4.96848665\n",
            "Iteration 176, loss = 4.94812466\n",
            "Iteration 177, loss = 4.94554124\n",
            "Iteration 178, loss = 4.94072492\n",
            "Iteration 179, loss = 4.93507984\n",
            "Iteration 180, loss = 4.92986745\n",
            "Iteration 181, loss = 4.93066861\n",
            "Iteration 182, loss = 4.92181686\n",
            "Iteration 183, loss = 4.92231131\n",
            "Iteration 184, loss = 4.91232481\n",
            "Iteration 185, loss = 4.90610696\n",
            "Iteration 186, loss = 4.90759610\n",
            "Iteration 187, loss = 4.90891813\n",
            "Iteration 188, loss = 4.89439200\n",
            "Iteration 189, loss = 4.89774765\n",
            "Iteration 190, loss = 4.88991157\n",
            "Iteration 191, loss = 4.87658062\n",
            "Iteration 192, loss = 4.88467418\n",
            "Iteration 193, loss = 4.87361871\n",
            "Iteration 194, loss = 4.87230190\n",
            "Iteration 195, loss = 4.86205149\n",
            "Iteration 196, loss = 4.87579190\n",
            "Iteration 197, loss = 4.86116062\n",
            "Iteration 198, loss = 4.86102563\n",
            "Iteration 199, loss = 4.85624966\n",
            "Iteration 200, loss = 4.84541209\n",
            "Iteration 201, loss = 4.86810612\n",
            "Iteration 202, loss = 4.82613151\n",
            "Iteration 203, loss = 4.82244239\n",
            "Iteration 204, loss = 4.81708977\n",
            "Iteration 205, loss = 4.83602852\n",
            "Iteration 206, loss = 4.80622603\n",
            "Iteration 207, loss = 4.81252998\n",
            "Iteration 208, loss = 4.80270378\n",
            "Iteration 209, loss = 4.78356294\n",
            "Iteration 210, loss = 4.78372364\n",
            "Iteration 211, loss = 4.79723409\n",
            "Iteration 212, loss = 4.82340306\n",
            "Iteration 213, loss = 4.76892544\n",
            "Iteration 214, loss = 4.77276530\n",
            "Iteration 215, loss = 4.75173102\n",
            "Iteration 216, loss = 4.81758065\n",
            "Iteration 217, loss = 4.74812383\n",
            "Iteration 218, loss = 4.75549127\n",
            "Iteration 219, loss = 4.73154532\n",
            "Iteration 220, loss = 4.75064340\n",
            "Iteration 221, loss = 4.72334705\n",
            "Iteration 222, loss = 4.73468973\n",
            "Iteration 223, loss = 4.70914236\n",
            "Iteration 224, loss = 4.71517055\n",
            "Iteration 225, loss = 4.71662524\n",
            "Iteration 226, loss = 4.69092726\n",
            "Iteration 227, loss = 4.68452603\n",
            "Iteration 228, loss = 4.69425426\n",
            "Iteration 229, loss = 4.66697813\n",
            "Iteration 230, loss = 4.66725254\n",
            "Iteration 231, loss = 4.68111910\n",
            "Iteration 232, loss = 4.66655826\n",
            "Iteration 233, loss = 4.68678907\n",
            "Iteration 234, loss = 4.66096784\n",
            "Iteration 235, loss = 4.63225035\n",
            "Iteration 236, loss = 4.65903482\n",
            "Iteration 237, loss = 4.63847035\n",
            "Iteration 238, loss = 4.61581972\n",
            "Iteration 239, loss = 4.60762704\n",
            "Iteration 240, loss = 4.60877992\n",
            "Iteration 241, loss = 4.60600705\n",
            "Iteration 242, loss = 4.58810820\n",
            "Iteration 243, loss = 4.63849499\n",
            "Iteration 244, loss = 4.58371321\n",
            "Iteration 245, loss = 4.57413257\n",
            "Iteration 246, loss = 4.55802199\n",
            "Iteration 247, loss = 4.55917296\n",
            "Iteration 248, loss = 4.54295288\n",
            "Iteration 249, loss = 4.54910014\n",
            "Iteration 250, loss = 4.56780632\n",
            "Iteration 251, loss = 4.53160766\n",
            "Iteration 252, loss = 4.51764778\n",
            "Iteration 253, loss = 4.52893257\n",
            "Iteration 254, loss = 4.49916938\n",
            "Iteration 255, loss = 4.51559944\n",
            "Iteration 256, loss = 4.48928420\n",
            "Iteration 257, loss = 4.48338086\n",
            "Iteration 258, loss = 4.46095748\n",
            "Iteration 259, loss = 4.47228139\n",
            "Iteration 260, loss = 4.48922883\n",
            "Iteration 261, loss = 4.44948858\n",
            "Iteration 262, loss = 4.46497064\n",
            "Iteration 263, loss = 4.44157036\n",
            "Iteration 264, loss = 4.43821301\n",
            "Iteration 265, loss = 4.42417450\n",
            "Iteration 266, loss = 4.41161257\n",
            "Iteration 267, loss = 4.38484474\n",
            "Iteration 268, loss = 4.39525064\n",
            "Iteration 269, loss = 4.48556645\n",
            "Iteration 270, loss = 4.38303002\n",
            "Iteration 271, loss = 4.36026276\n",
            "Iteration 272, loss = 4.36175956\n",
            "Iteration 273, loss = 4.37156012\n",
            "Iteration 274, loss = 4.34933644\n",
            "Iteration 275, loss = 4.37276262\n",
            "Iteration 276, loss = 4.33140576\n",
            "Iteration 277, loss = 4.32787648\n",
            "Iteration 278, loss = 4.36784719\n",
            "Iteration 279, loss = 4.33687791\n",
            "Iteration 280, loss = 4.29986556\n",
            "Iteration 281, loss = 4.29472525\n",
            "Iteration 282, loss = 4.29147977\n",
            "Iteration 283, loss = 4.26286684\n",
            "Iteration 284, loss = 4.24438445\n",
            "Iteration 285, loss = 4.24126238\n",
            "Iteration 286, loss = 4.27394833\n",
            "Iteration 287, loss = 4.26877899\n",
            "Iteration 288, loss = 4.28996847\n",
            "Iteration 289, loss = 4.26260304\n",
            "Iteration 290, loss = 4.20417065\n",
            "Iteration 291, loss = 4.20047127\n",
            "Iteration 292, loss = 4.20118039\n",
            "Iteration 293, loss = 4.15519206\n",
            "Iteration 294, loss = 4.19245899\n",
            "Iteration 295, loss = 4.17037031\n",
            "Iteration 296, loss = 4.14650875\n",
            "Iteration 297, loss = 4.21546525\n",
            "Iteration 298, loss = 4.10401697\n",
            "Iteration 299, loss = 4.16289827\n",
            "Iteration 300, loss = 4.09162825\n",
            "Iteration 301, loss = 4.10815558\n",
            "Iteration 302, loss = 4.12326158\n",
            "Iteration 303, loss = 4.09412293\n",
            "Iteration 304, loss = 4.13851767\n",
            "Iteration 305, loss = 4.07179193\n",
            "Iteration 306, loss = 4.05163823\n",
            "Iteration 307, loss = 4.05360394\n",
            "Iteration 308, loss = 3.99466739\n",
            "Iteration 309, loss = 4.01160327\n",
            "Iteration 310, loss = 4.13220306\n",
            "Iteration 311, loss = 4.01266997\n",
            "Iteration 312, loss = 3.99050248\n",
            "Iteration 313, loss = 4.00364464\n",
            "Iteration 314, loss = 3.99684081\n",
            "Iteration 315, loss = 3.99811193\n",
            "Iteration 316, loss = 3.98686337\n",
            "Iteration 317, loss = 3.90946195\n",
            "Iteration 318, loss = 3.94191221\n",
            "Iteration 319, loss = 3.90613007\n",
            "Iteration 320, loss = 3.95986784\n",
            "Iteration 321, loss = 3.87176032\n",
            "Iteration 322, loss = 3.91133285\n",
            "Iteration 323, loss = 3.92736519\n",
            "Iteration 324, loss = 3.95370719\n",
            "Iteration 325, loss = 3.90235092\n",
            "Iteration 326, loss = 3.92919290\n",
            "Iteration 327, loss = 3.85312102\n",
            "Iteration 328, loss = 3.81024476\n",
            "Iteration 329, loss = 3.80916690\n",
            "Iteration 330, loss = 3.77012203\n",
            "Iteration 331, loss = 3.79146861\n",
            "Iteration 332, loss = 3.82641754\n",
            "Iteration 333, loss = 3.86772818\n",
            "Iteration 334, loss = 3.76605550\n",
            "Iteration 335, loss = 3.74687562\n",
            "Iteration 336, loss = 3.77189602\n",
            "Iteration 337, loss = 3.83082463\n",
            "Iteration 338, loss = 3.69321038\n",
            "Iteration 339, loss = 3.75251588\n",
            "Iteration 340, loss = 3.72784409\n",
            "Iteration 341, loss = 3.65537454\n",
            "Iteration 342, loss = 3.77398649\n",
            "Iteration 343, loss = 3.75914008\n",
            "Iteration 344, loss = 3.65518505\n",
            "Iteration 345, loss = 3.62986253\n",
            "Iteration 346, loss = 3.64758767\n",
            "Iteration 347, loss = 3.67913634\n",
            "Iteration 348, loss = 3.60046750\n",
            "Iteration 349, loss = 3.57783706\n",
            "Iteration 350, loss = 3.56421919\n",
            "Iteration 351, loss = 3.57944106\n",
            "Iteration 352, loss = 3.64112517\n",
            "Iteration 353, loss = 3.65592049\n",
            "Iteration 354, loss = 3.53757407\n",
            "Iteration 355, loss = 3.61819608\n",
            "Iteration 356, loss = 3.53968907\n",
            "Iteration 357, loss = 3.52751561\n",
            "Iteration 358, loss = 3.55579333\n",
            "Iteration 359, loss = 3.41267322\n",
            "Iteration 360, loss = 3.43467327\n",
            "Iteration 361, loss = 3.54182799\n",
            "Iteration 362, loss = 3.46677882\n",
            "Iteration 363, loss = 3.37784621\n",
            "Iteration 364, loss = 3.47699117\n",
            "Iteration 365, loss = 3.45202267\n",
            "Iteration 366, loss = 3.32965326\n",
            "Iteration 367, loss = 3.41673604\n",
            "Iteration 368, loss = 3.29398236\n",
            "Iteration 369, loss = 3.40165489\n",
            "Iteration 370, loss = 3.40663190\n",
            "Iteration 371, loss = 3.30854384\n",
            "Iteration 372, loss = 3.29854597\n",
            "Iteration 373, loss = 3.35730037\n",
            "Iteration 374, loss = 3.27563082\n",
            "Iteration 375, loss = 3.40093461\n",
            "Iteration 376, loss = 3.29939492\n",
            "Iteration 377, loss = 3.38282358\n",
            "Iteration 378, loss = 3.33941171\n",
            "Iteration 379, loss = 3.23687839\n",
            "Iteration 380, loss = 3.32910787\n",
            "Iteration 381, loss = 3.21428093\n",
            "Iteration 382, loss = 3.19165057\n",
            "Iteration 383, loss = 3.34559337\n",
            "Iteration 384, loss = 3.29457557\n",
            "Iteration 385, loss = 3.14852317\n",
            "Iteration 386, loss = 3.19838983\n",
            "Iteration 387, loss = 3.12609260\n",
            "Iteration 388, loss = 3.18305498\n",
            "Iteration 389, loss = 3.12101063\n",
            "Iteration 390, loss = 3.16030079\n",
            "Iteration 391, loss = 3.28280179\n",
            "Iteration 392, loss = 3.01449706\n",
            "Iteration 393, loss = 2.96951930\n",
            "Iteration 394, loss = 3.10910777\n",
            "Iteration 395, loss = 2.99406415\n",
            "Iteration 396, loss = 3.10289469\n",
            "Iteration 397, loss = 3.07028955\n",
            "Iteration 398, loss = 2.93685082\n",
            "Iteration 399, loss = 3.19344149\n",
            "Iteration 400, loss = 2.97230228\n",
            "Iteration 401, loss = 2.91968193\n",
            "Iteration 402, loss = 3.11760773\n",
            "Iteration 403, loss = 2.95231515\n",
            "Iteration 404, loss = 2.91740245\n",
            "Iteration 405, loss = 2.94413881\n",
            "Iteration 406, loss = 2.87771357\n",
            "Iteration 407, loss = 2.88731032\n",
            "Iteration 408, loss = 2.88129419\n",
            "Iteration 409, loss = 2.91930142\n",
            "Iteration 410, loss = 2.83889534\n",
            "Iteration 411, loss = 2.87398071\n",
            "Iteration 412, loss = 3.02403806\n",
            "Iteration 413, loss = 2.82332264\n",
            "Iteration 414, loss = 2.98129954\n",
            "Iteration 415, loss = 2.82656253\n",
            "Iteration 416, loss = 2.84723559\n",
            "Iteration 417, loss = 2.67030936\n",
            "Iteration 418, loss = 2.86784239\n",
            "Iteration 419, loss = 2.75804972\n",
            "Iteration 420, loss = 2.69593554\n",
            "Iteration 421, loss = 2.93532803\n",
            "Iteration 422, loss = 2.71007223\n",
            "Iteration 423, loss = 2.94649685\n",
            "Iteration 424, loss = 2.60893844\n",
            "Iteration 425, loss = 2.78715226\n",
            "Iteration 426, loss = 2.81518574\n",
            "Iteration 427, loss = 2.66029469\n",
            "Iteration 428, loss = 2.65119792\n",
            "Iteration 429, loss = 2.85607035\n",
            "Iteration 430, loss = 2.65225428\n",
            "Iteration 431, loss = 2.58126973\n",
            "Iteration 432, loss = 2.64852139\n",
            "Iteration 433, loss = 2.61655369\n",
            "Iteration 434, loss = 2.64453239\n",
            "Iteration 435, loss = 2.51363644\n",
            "Iteration 436, loss = 2.75064427\n",
            "Iteration 437, loss = 2.64702133\n",
            "Iteration 438, loss = 2.47239259\n",
            "Iteration 439, loss = 2.50264767\n",
            "Iteration 440, loss = 2.50178342\n",
            "Iteration 441, loss = 2.35328374\n",
            "Iteration 442, loss = 2.51972268\n",
            "Iteration 443, loss = 2.60535344\n",
            "Iteration 444, loss = 2.54285420\n",
            "Iteration 445, loss = 2.58276528\n",
            "Iteration 446, loss = 2.49329974\n",
            "Iteration 447, loss = 2.41505353\n",
            "Iteration 448, loss = 2.50384913\n",
            "Iteration 449, loss = 2.36927337\n",
            "Iteration 450, loss = 2.60706639\n",
            "Iteration 451, loss = 2.56204903\n",
            "Iteration 452, loss = 2.32643152\n",
            "Iteration 453, loss = 2.34376173\n",
            "Iteration 454, loss = 2.35496372\n",
            "Iteration 455, loss = 2.63210059\n",
            "Iteration 456, loss = 2.32739371\n",
            "Iteration 457, loss = 2.37895187\n",
            "Iteration 458, loss = 2.47355732\n",
            "Iteration 459, loss = 2.31619067\n",
            "Iteration 460, loss = 2.51248810\n",
            "Iteration 461, loss = 2.15995040\n",
            "Iteration 462, loss = 2.32654292\n",
            "Iteration 463, loss = 2.11499152\n",
            "Iteration 464, loss = 2.31316926\n",
            "Iteration 465, loss = 2.41447645\n",
            "Iteration 466, loss = 2.35275306\n",
            "Iteration 467, loss = 2.14219461\n",
            "Iteration 468, loss = 2.26115978\n",
            "Iteration 469, loss = 2.15970300\n",
            "Iteration 470, loss = 2.11657612\n",
            "Iteration 471, loss = 2.14036335\n",
            "Iteration 472, loss = 2.26543758\n",
            "Iteration 473, loss = 2.18775392\n",
            "Iteration 474, loss = 2.25496615\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 475, loss = 1.96103806\n",
            "Iteration 476, loss = 1.79594084\n",
            "Iteration 477, loss = 1.72517913\n",
            "Iteration 478, loss = 1.68967045\n",
            "Iteration 479, loss = 1.68058879\n",
            "Iteration 480, loss = 1.66762050\n",
            "Iteration 481, loss = 1.66212723\n",
            "Iteration 482, loss = 1.65745200\n",
            "Iteration 483, loss = 1.65232767\n",
            "Iteration 484, loss = 1.64722343\n",
            "Iteration 485, loss = 1.63965365\n",
            "Iteration 486, loss = 1.63351074\n",
            "Iteration 487, loss = 1.63023633\n",
            "Iteration 488, loss = 1.62693160\n",
            "Iteration 489, loss = 1.61840737\n",
            "Iteration 490, loss = 1.61148914\n",
            "Iteration 491, loss = 1.61290430\n",
            "Iteration 492, loss = 1.60930147\n",
            "Iteration 493, loss = 1.60191857\n",
            "Iteration 494, loss = 1.59700756\n",
            "Iteration 495, loss = 1.59252972\n",
            "Iteration 496, loss = 1.59330707\n",
            "Iteration 497, loss = 1.58345697\n",
            "Iteration 498, loss = 1.58115048\n",
            "Iteration 499, loss = 1.57367325\n",
            "Iteration 500, loss = 1.57100413\n",
            "Iteration 501, loss = 1.56615256\n",
            "Iteration 502, loss = 1.56794148\n",
            "Iteration 503, loss = 1.56010158\n",
            "Iteration 504, loss = 1.55186212\n",
            "Iteration 505, loss = 1.55495848\n",
            "Iteration 506, loss = 1.55669127\n",
            "Iteration 507, loss = 1.54940983\n",
            "Iteration 508, loss = 1.53949076\n",
            "Iteration 509, loss = 1.53404089\n",
            "Iteration 510, loss = 1.53738352\n",
            "Iteration 511, loss = 1.53233664\n",
            "Iteration 512, loss = 1.52892983\n",
            "Iteration 513, loss = 1.51623431\n",
            "Iteration 514, loss = 1.52108136\n",
            "Iteration 515, loss = 1.51594851\n",
            "Iteration 516, loss = 1.51850049\n",
            "Iteration 517, loss = 1.51041539\n",
            "Iteration 518, loss = 1.50141122\n",
            "Iteration 519, loss = 1.49852281\n",
            "Iteration 520, loss = 1.50059928\n",
            "Iteration 521, loss = 1.49188000\n",
            "Iteration 522, loss = 1.48579162\n",
            "Iteration 523, loss = 1.48909291\n",
            "Iteration 524, loss = 1.48621473\n",
            "Iteration 525, loss = 1.47715015\n",
            "Iteration 526, loss = 1.47889925\n",
            "Iteration 527, loss = 1.47263235\n",
            "Iteration 528, loss = 1.47144417\n",
            "Iteration 529, loss = 1.46135577\n",
            "Iteration 530, loss = 1.46175009\n",
            "Iteration 531, loss = 1.45536388\n",
            "Iteration 532, loss = 1.45951090\n",
            "Iteration 533, loss = 1.44981553\n",
            "Iteration 534, loss = 1.44494126\n",
            "Iteration 535, loss = 1.44639853\n",
            "Iteration 536, loss = 1.44265987\n",
            "Iteration 537, loss = 1.44014717\n",
            "Iteration 538, loss = 1.43045016\n",
            "Iteration 539, loss = 1.43347342\n",
            "Iteration 540, loss = 1.42658345\n",
            "Iteration 541, loss = 1.43100104\n",
            "Iteration 542, loss = 1.42530987\n",
            "Iteration 543, loss = 1.41854144\n",
            "Iteration 544, loss = 1.41604943\n",
            "Iteration 545, loss = 1.41685085\n",
            "Iteration 546, loss = 1.40967962\n",
            "Iteration 547, loss = 1.40060519\n",
            "Iteration 548, loss = 1.40295546\n",
            "Iteration 549, loss = 1.40030476\n",
            "Iteration 550, loss = 1.40417405\n",
            "Iteration 551, loss = 1.39530632\n",
            "Iteration 552, loss = 1.38113587\n",
            "Iteration 553, loss = 1.38944686\n",
            "Iteration 554, loss = 1.38383401\n",
            "Iteration 555, loss = 1.37818380\n",
            "Iteration 556, loss = 1.37145149\n",
            "Iteration 557, loss = 1.37462925\n",
            "Iteration 558, loss = 1.37295724\n",
            "Iteration 559, loss = 1.37254511\n",
            "Iteration 560, loss = 1.36775357\n",
            "Iteration 561, loss = 1.36301706\n",
            "Iteration 562, loss = 1.35955397\n",
            "Iteration 563, loss = 1.36073577\n",
            "Iteration 564, loss = 1.35303327\n",
            "Iteration 565, loss = 1.35157411\n",
            "Iteration 566, loss = 1.34757546\n",
            "Iteration 567, loss = 1.34329194\n",
            "Iteration 568, loss = 1.34013311\n",
            "Iteration 569, loss = 1.33910567\n",
            "Iteration 570, loss = 1.33560801\n",
            "Iteration 571, loss = 1.33081311\n",
            "Iteration 572, loss = 1.32806277\n",
            "Iteration 573, loss = 1.32420751\n",
            "Iteration 574, loss = 1.31853432\n",
            "Iteration 575, loss = 1.33012599\n",
            "Iteration 576, loss = 1.31937041\n",
            "Iteration 577, loss = 1.32199185\n",
            "Iteration 578, loss = 1.30571523\n",
            "Iteration 579, loss = 1.30436346\n",
            "Iteration 580, loss = 1.30884781\n",
            "Iteration 581, loss = 1.29827247\n",
            "Iteration 582, loss = 1.29290644\n",
            "Iteration 583, loss = 1.29602390\n",
            "Iteration 584, loss = 1.28992682\n",
            "Iteration 585, loss = 1.28829267\n",
            "Iteration 586, loss = 1.28318302\n",
            "Iteration 587, loss = 1.28291902\n",
            "Iteration 588, loss = 1.27508323\n",
            "Iteration 589, loss = 1.27841364\n",
            "Iteration 590, loss = 1.28031303\n",
            "Iteration 591, loss = 1.27205309\n",
            "Iteration 592, loss = 1.27338897\n",
            "Iteration 593, loss = 1.26605697\n",
            "Iteration 594, loss = 1.27164284\n",
            "Iteration 595, loss = 1.26814932\n",
            "Iteration 596, loss = 1.25318692\n",
            "Iteration 597, loss = 1.26396692\n",
            "Iteration 598, loss = 1.25873697\n",
            "Iteration 599, loss = 1.24593058\n",
            "Iteration 600, loss = 1.24796348\n",
            "Iteration 601, loss = 1.24127714\n",
            "Iteration 602, loss = 1.24558254\n",
            "Iteration 603, loss = 1.24122673\n",
            "Iteration 604, loss = 1.23531534\n",
            "Iteration 605, loss = 1.22991655\n",
            "Iteration 606, loss = 1.22809397\n",
            "Iteration 607, loss = 1.22295917\n",
            "Iteration 608, loss = 1.23112512\n",
            "Iteration 609, loss = 1.22158848\n",
            "Iteration 610, loss = 1.22172099\n",
            "Iteration 611, loss = 1.21719449\n",
            "Iteration 612, loss = 1.21640318\n",
            "Iteration 613, loss = 1.20632017\n",
            "Iteration 614, loss = 1.20887252\n",
            "Iteration 615, loss = 1.20720481\n",
            "Iteration 616, loss = 1.20641044\n",
            "Iteration 617, loss = 1.20267487\n",
            "Iteration 618, loss = 1.19543995\n",
            "Iteration 619, loss = 1.19891619\n",
            "Iteration 620, loss = 1.19139826\n",
            "Iteration 621, loss = 1.18159129\n",
            "Iteration 622, loss = 1.18745677\n",
            "Iteration 623, loss = 1.17676815\n",
            "Iteration 624, loss = 1.18118237\n",
            "Iteration 625, loss = 1.17920322\n",
            "Iteration 626, loss = 1.17425588\n",
            "Iteration 627, loss = 1.17381501\n",
            "Iteration 628, loss = 1.17434134\n",
            "Iteration 629, loss = 1.16753573\n",
            "Iteration 630, loss = 1.16443793\n",
            "Iteration 631, loss = 1.16250661\n",
            "Iteration 632, loss = 1.16318541\n",
            "Iteration 633, loss = 1.15733698\n",
            "Iteration 634, loss = 1.15900661\n",
            "Iteration 635, loss = 1.15916957\n",
            "Iteration 636, loss = 1.15353295\n",
            "Iteration 637, loss = 1.14358458\n",
            "Iteration 638, loss = 1.13927838\n",
            "Iteration 639, loss = 1.14674362\n",
            "Iteration 640, loss = 1.14108721\n",
            "Iteration 641, loss = 1.12932133\n",
            "Iteration 642, loss = 1.14133250\n",
            "Iteration 643, loss = 1.13363025\n",
            "Iteration 644, loss = 1.13508650\n",
            "Iteration 645, loss = 1.12810773\n",
            "Iteration 646, loss = 1.12575341\n",
            "Iteration 647, loss = 1.12777871\n",
            "Iteration 648, loss = 1.11982472\n",
            "Iteration 649, loss = 1.11626882\n",
            "Iteration 650, loss = 1.11666740\n",
            "Iteration 651, loss = 1.11223604\n",
            "Iteration 652, loss = 1.11378215\n",
            "Iteration 653, loss = 1.10777314\n",
            "Iteration 654, loss = 1.09945623\n",
            "Iteration 655, loss = 1.10322531\n",
            "Iteration 656, loss = 1.10179770\n",
            "Iteration 657, loss = 1.09297659\n",
            "Iteration 658, loss = 1.10190802\n",
            "Iteration 659, loss = 1.08366449\n",
            "Iteration 660, loss = 1.08930675\n",
            "Iteration 661, loss = 1.08047311\n",
            "Iteration 662, loss = 1.08113623\n",
            "Iteration 663, loss = 1.07337667\n",
            "Iteration 664, loss = 1.08086032\n",
            "Iteration 665, loss = 1.07198111\n",
            "Iteration 666, loss = 1.07601716\n",
            "Iteration 667, loss = 1.06467525\n",
            "Iteration 668, loss = 1.06772511\n",
            "Iteration 669, loss = 1.06428370\n",
            "Iteration 670, loss = 1.05609550\n",
            "Iteration 671, loss = 1.06429457\n",
            "Iteration 672, loss = 1.05886781\n",
            "Iteration 673, loss = 1.05139590\n",
            "Iteration 674, loss = 1.05713800\n",
            "Iteration 675, loss = 1.05391633\n",
            "Iteration 676, loss = 1.04512846\n",
            "Iteration 677, loss = 1.04216879\n",
            "Iteration 678, loss = 1.04615758\n",
            "Iteration 679, loss = 1.03635028\n",
            "Iteration 680, loss = 1.03840966\n",
            "Iteration 681, loss = 1.03813876\n",
            "Iteration 682, loss = 1.03354887\n",
            "Iteration 683, loss = 1.03279669\n",
            "Iteration 684, loss = 1.03585423\n",
            "Iteration 685, loss = 1.03672161\n",
            "Iteration 686, loss = 1.02583150\n",
            "Iteration 687, loss = 1.01735365\n",
            "Iteration 688, loss = 1.02575881\n",
            "Iteration 689, loss = 1.02334920\n",
            "Iteration 690, loss = 1.03185981\n",
            "Iteration 691, loss = 1.01232918\n",
            "Iteration 692, loss = 1.01044670\n",
            "Iteration 693, loss = 1.01136889\n",
            "Iteration 694, loss = 1.02228427\n",
            "Iteration 695, loss = 1.00410358\n",
            "Iteration 696, loss = 1.02217986\n",
            "Iteration 697, loss = 0.99547798\n",
            "Iteration 698, loss = 0.99441900\n",
            "Iteration 699, loss = 0.99196073\n",
            "Iteration 700, loss = 0.99481743\n",
            "Iteration 701, loss = 0.99110982\n",
            "Iteration 702, loss = 0.98940912\n",
            "Iteration 703, loss = 0.98940513\n",
            "Iteration 704, loss = 0.98434915\n",
            "Iteration 705, loss = 0.98139962\n",
            "Iteration 706, loss = 0.97167086\n",
            "Iteration 707, loss = 0.97780460\n",
            "Iteration 708, loss = 0.97930396\n",
            "Iteration 709, loss = 0.97875345\n",
            "Iteration 710, loss = 0.96920331\n",
            "Iteration 711, loss = 0.96341029\n",
            "Iteration 712, loss = 0.97033137\n",
            "Iteration 713, loss = 0.95769995\n",
            "Iteration 714, loss = 0.96695254\n",
            "Iteration 715, loss = 0.97031019\n",
            "Iteration 716, loss = 0.96962202\n",
            "Iteration 717, loss = 0.95942691\n",
            "Iteration 718, loss = 0.96085025\n",
            "Iteration 719, loss = 0.95196301\n",
            "Iteration 720, loss = 0.96518649\n",
            "Iteration 721, loss = 0.94932986\n",
            "Iteration 722, loss = 0.95265183\n",
            "Iteration 723, loss = 0.95629224\n",
            "Iteration 724, loss = 0.93507828\n",
            "Iteration 725, loss = 0.93220508\n",
            "Iteration 726, loss = 0.93391017\n",
            "Iteration 727, loss = 0.94277748\n",
            "Iteration 728, loss = 0.93586839\n",
            "Iteration 729, loss = 0.92987077\n",
            "Iteration 730, loss = 0.93052123\n",
            "Iteration 731, loss = 0.92209721\n",
            "Iteration 732, loss = 0.90869432\n",
            "Iteration 733, loss = 0.91925997\n",
            "Iteration 734, loss = 0.92389713\n",
            "Iteration 735, loss = 0.91579645\n",
            "Iteration 736, loss = 0.90970092\n",
            "Iteration 737, loss = 0.90800102\n",
            "Iteration 738, loss = 0.91055176\n",
            "Iteration 739, loss = 0.91637467\n",
            "Iteration 740, loss = 0.90572652\n",
            "Iteration 741, loss = 0.90463615\n",
            "Iteration 742, loss = 0.89254251\n",
            "Iteration 743, loss = 0.89886489\n",
            "Iteration 744, loss = 0.89441290\n",
            "Iteration 745, loss = 0.88442375\n",
            "Iteration 746, loss = 0.89718237\n",
            "Iteration 747, loss = 0.88958207\n",
            "Iteration 748, loss = 0.88921268\n",
            "Iteration 749, loss = 0.88041734\n",
            "Iteration 750, loss = 0.88900387\n",
            "Iteration 751, loss = 0.89495312\n",
            "Iteration 752, loss = 0.88344108\n",
            "Iteration 753, loss = 0.87691781\n",
            "Iteration 754, loss = 0.87427858\n",
            "Iteration 755, loss = 0.87351040\n",
            "Iteration 756, loss = 0.87527000\n",
            "Iteration 757, loss = 0.87172975\n",
            "Iteration 758, loss = 0.89052352\n",
            "Iteration 759, loss = 0.87566918\n",
            "Iteration 760, loss = 0.86993435\n",
            "Iteration 761, loss = 0.89055615\n",
            "Iteration 762, loss = 0.86135709\n",
            "Iteration 763, loss = 0.86533680\n",
            "Iteration 764, loss = 0.85893129\n",
            "Iteration 765, loss = 0.86363675\n",
            "Iteration 766, loss = 0.87100987\n",
            "Iteration 767, loss = 0.85241727\n",
            "Iteration 768, loss = 0.85670013\n",
            "Iteration 769, loss = 0.86785130\n",
            "Iteration 770, loss = 0.84692333\n",
            "Iteration 771, loss = 0.85321322\n",
            "Iteration 772, loss = 0.83485728\n",
            "Iteration 773, loss = 0.84248844\n",
            "Iteration 774, loss = 0.84854234\n",
            "Iteration 775, loss = 0.83595429\n",
            "Iteration 776, loss = 0.83962910\n",
            "Iteration 777, loss = 0.83978678\n",
            "Iteration 778, loss = 0.84207077\n",
            "Iteration 779, loss = 0.81734191\n",
            "Iteration 780, loss = 0.82247766\n",
            "Iteration 781, loss = 0.82454281\n",
            "Iteration 782, loss = 0.81703535\n",
            "Iteration 783, loss = 0.84959159\n",
            "Iteration 784, loss = 0.81913038\n",
            "Iteration 785, loss = 0.82308678\n",
            "Iteration 786, loss = 0.81151779\n",
            "Iteration 787, loss = 0.81462055\n",
            "Iteration 788, loss = 0.83098685\n",
            "Iteration 789, loss = 0.83075738\n",
            "Iteration 790, loss = 0.81080679\n",
            "Iteration 791, loss = 0.80138122\n",
            "Iteration 792, loss = 0.81588390\n",
            "Iteration 793, loss = 0.80279654\n",
            "Iteration 794, loss = 0.79559942\n",
            "Iteration 795, loss = 0.81042543\n",
            "Iteration 796, loss = 0.80020332\n",
            "Iteration 797, loss = 0.80666913\n",
            "Iteration 798, loss = 0.79917753\n",
            "Iteration 799, loss = 0.79794397\n",
            "Iteration 800, loss = 0.79728290\n",
            "Iteration 801, loss = 0.78177537\n",
            "Iteration 802, loss = 0.80059793\n",
            "Iteration 803, loss = 0.80180849\n",
            "Iteration 804, loss = 0.79009724\n",
            "Iteration 805, loss = 0.78446180\n",
            "Iteration 806, loss = 0.77425757\n",
            "Iteration 807, loss = 0.78424003\n",
            "Iteration 808, loss = 0.77630862\n",
            "Iteration 809, loss = 0.77461385\n",
            "Iteration 810, loss = 0.76585651\n",
            "Iteration 811, loss = 0.77268332\n",
            "Iteration 812, loss = 0.76532176\n",
            "Iteration 813, loss = 0.77167979\n",
            "Iteration 814, loss = 0.76028022\n",
            "Iteration 815, loss = 0.75532706\n",
            "Iteration 816, loss = 0.76223418\n",
            "Iteration 817, loss = 0.75255301\n",
            "Iteration 818, loss = 0.75105629\n",
            "Iteration 819, loss = 0.77459850\n",
            "Iteration 820, loss = 0.74763371\n",
            "Iteration 821, loss = 0.75803692\n",
            "Iteration 822, loss = 0.78023036\n",
            "Iteration 823, loss = 0.76579863\n",
            "Iteration 824, loss = 0.76202017\n",
            "Iteration 825, loss = 0.74658847\n",
            "Iteration 826, loss = 0.74628294\n",
            "Iteration 827, loss = 0.74971506\n",
            "Iteration 828, loss = 0.73549037\n",
            "Iteration 829, loss = 0.74533071\n",
            "Iteration 830, loss = 0.72826525\n",
            "Iteration 831, loss = 0.74624028\n",
            "Iteration 832, loss = 0.75166172\n",
            "Iteration 833, loss = 0.75294717\n",
            "Iteration 834, loss = 0.74869055\n",
            "Iteration 835, loss = 0.75598236\n",
            "Iteration 836, loss = 0.74068497\n",
            "Iteration 837, loss = 0.73845895\n",
            "Iteration 838, loss = 0.73550853\n",
            "Iteration 839, loss = 0.72073332\n",
            "Iteration 840, loss = 0.72449603\n",
            "Iteration 841, loss = 0.70298671\n",
            "Iteration 842, loss = 0.73265804\n",
            "Iteration 843, loss = 0.71517852\n",
            "Iteration 844, loss = 0.70459019\n",
            "Iteration 845, loss = 0.70252490\n",
            "Iteration 846, loss = 0.72919951\n",
            "Iteration 847, loss = 0.70643888\n",
            "Iteration 848, loss = 0.72293982\n",
            "Iteration 849, loss = 0.70745349\n",
            "Iteration 850, loss = 0.71267738\n",
            "Iteration 851, loss = 0.71741821\n",
            "Iteration 852, loss = 0.77041180\n",
            "Iteration 853, loss = 0.70011564\n",
            "Iteration 854, loss = 0.69768402\n",
            "Iteration 855, loss = 0.69423449\n",
            "Iteration 856, loss = 0.74857055\n",
            "Iteration 857, loss = 0.70852800\n",
            "Iteration 858, loss = 0.70196668\n",
            "Iteration 859, loss = 0.68588913\n",
            "Iteration 860, loss = 0.72257469\n",
            "Iteration 861, loss = 0.68385903\n",
            "Iteration 862, loss = 0.68070635\n",
            "Iteration 863, loss = 0.69882183\n",
            "Iteration 864, loss = 0.67948475\n",
            "Iteration 865, loss = 0.67846755\n",
            "Iteration 866, loss = 0.68614899\n",
            "Iteration 867, loss = 0.66933879\n",
            "Iteration 868, loss = 0.67533248\n",
            "Iteration 869, loss = 0.67120022\n",
            "Iteration 870, loss = 0.69003223\n",
            "Iteration 871, loss = 0.67717539\n",
            "Iteration 872, loss = 0.66459445\n",
            "Iteration 873, loss = 0.66782550\n",
            "Iteration 874, loss = 0.68560088\n",
            "Iteration 875, loss = 0.67423062\n",
            "Iteration 876, loss = 0.64946191\n",
            "Iteration 877, loss = 0.68601091\n",
            "Iteration 878, loss = 0.65475516\n",
            "Iteration 879, loss = 0.65868803\n",
            "Iteration 880, loss = 0.69178677\n",
            "Iteration 881, loss = 0.69043329\n",
            "Iteration 882, loss = 0.68442357\n",
            "Iteration 883, loss = 0.65014410\n",
            "Iteration 884, loss = 0.66368286\n",
            "Iteration 885, loss = 0.64461281\n",
            "Iteration 886, loss = 0.63152150\n",
            "Iteration 887, loss = 0.64402894\n",
            "Iteration 888, loss = 0.64769275\n",
            "Iteration 889, loss = 0.66202089\n",
            "Iteration 890, loss = 0.63699601\n",
            "Iteration 891, loss = 0.66284885\n",
            "Iteration 892, loss = 0.65318879\n",
            "Iteration 893, loss = 0.63365341\n",
            "Iteration 894, loss = 0.63798420\n",
            "Iteration 895, loss = 0.64110526\n",
            "Iteration 896, loss = 0.63499188\n",
            "Iteration 897, loss = 0.63268371\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 898, loss = 0.61114083\n",
            "Iteration 899, loss = 0.58094385\n",
            "Iteration 900, loss = 0.56717201\n",
            "Iteration 901, loss = 0.56384313\n",
            "Iteration 902, loss = 0.55998178\n",
            "Iteration 903, loss = 0.56065415\n",
            "Iteration 904, loss = 0.56022614\n",
            "Iteration 905, loss = 0.56004582\n",
            "Iteration 906, loss = 0.55849333\n",
            "Iteration 907, loss = 0.55903326\n",
            "Iteration 908, loss = 0.55909643\n",
            "Iteration 909, loss = 0.55717145\n",
            "Iteration 910, loss = 0.55579728\n",
            "Iteration 911, loss = 0.56137805\n",
            "Iteration 912, loss = 0.55688178\n",
            "Iteration 913, loss = 0.55687504\n",
            "Iteration 914, loss = 0.55739971\n",
            "Iteration 915, loss = 0.55611649\n",
            "Iteration 916, loss = 0.55439431\n",
            "Iteration 917, loss = 0.55515191\n",
            "Iteration 918, loss = 0.55627211\n",
            "Iteration 919, loss = 0.55373662\n",
            "Iteration 920, loss = 0.55293310\n",
            "Iteration 921, loss = 0.55353314\n",
            "Iteration 922, loss = 0.55206466\n",
            "Iteration 923, loss = 0.55253360\n",
            "Iteration 924, loss = 0.55189542\n",
            "Iteration 925, loss = 0.55284074\n",
            "Iteration 926, loss = 0.55158675\n",
            "Iteration 927, loss = 0.55140029\n",
            "Iteration 928, loss = 0.55125098\n",
            "Iteration 929, loss = 0.55012948\n",
            "Iteration 930, loss = 0.55112787\n",
            "Iteration 931, loss = 0.55180796\n",
            "Iteration 932, loss = 0.55223982\n",
            "Iteration 933, loss = 0.55155764\n",
            "Iteration 934, loss = 0.54948447\n",
            "Iteration 935, loss = 0.55096371\n",
            "Iteration 936, loss = 0.54875942\n",
            "Iteration 937, loss = 0.54837887\n",
            "Iteration 938, loss = 0.54813133\n",
            "Iteration 939, loss = 0.54785257\n",
            "Iteration 940, loss = 0.54738302\n",
            "Iteration 941, loss = 0.54497884\n",
            "Iteration 942, loss = 0.54552585\n",
            "Iteration 943, loss = 0.54817626\n",
            "Iteration 944, loss = 0.54514585\n",
            "Iteration 945, loss = 0.54463463\n",
            "Iteration 946, loss = 0.54468852\n",
            "Iteration 947, loss = 0.54445236\n",
            "Iteration 948, loss = 0.54580911\n",
            "Iteration 949, loss = 0.54349546\n",
            "Iteration 950, loss = 0.54404262\n",
            "Iteration 951, loss = 0.54231401\n",
            "Iteration 952, loss = 0.54180570\n",
            "Iteration 953, loss = 0.54406170\n",
            "Iteration 954, loss = 0.54314472\n",
            "Iteration 955, loss = 0.54362513\n",
            "Iteration 956, loss = 0.54414212\n",
            "Iteration 957, loss = 0.54174089\n",
            "Iteration 958, loss = 0.54292559\n",
            "Iteration 959, loss = 0.54081124\n",
            "Iteration 960, loss = 0.54349091\n",
            "Iteration 961, loss = 0.54341371\n",
            "Iteration 962, loss = 0.54060256\n",
            "Iteration 963, loss = 0.53907574\n",
            "Iteration 964, loss = 0.53986193\n",
            "Iteration 965, loss = 0.54039494\n",
            "Iteration 966, loss = 0.53853889\n",
            "Iteration 967, loss = 0.53826125\n",
            "Iteration 968, loss = 0.53865349\n",
            "Iteration 969, loss = 0.53795209\n",
            "Iteration 970, loss = 0.54017075\n",
            "Iteration 971, loss = 0.53737693\n",
            "Iteration 972, loss = 0.54015678\n",
            "Iteration 973, loss = 0.53622787\n",
            "Iteration 974, loss = 0.53950739\n",
            "Iteration 975, loss = 0.53871328\n",
            "Iteration 976, loss = 0.53602304\n",
            "Iteration 977, loss = 0.53644723\n",
            "Iteration 978, loss = 0.53719613\n",
            "Iteration 979, loss = 0.53815078\n",
            "Iteration 980, loss = 0.53735850\n",
            "Iteration 981, loss = 0.53659977\n",
            "Iteration 982, loss = 0.53501588\n",
            "Iteration 983, loss = 0.53591642\n",
            "Iteration 984, loss = 0.53530756\n",
            "Iteration 985, loss = 0.53333083\n",
            "Iteration 986, loss = 0.53312516\n",
            "Iteration 987, loss = 0.53404769\n",
            "Iteration 988, loss = 0.53285978\n",
            "Iteration 989, loss = 0.53225188\n",
            "Iteration 990, loss = 0.53078759\n",
            "Iteration 991, loss = 0.53312986\n",
            "Iteration 992, loss = 0.53034612\n",
            "Iteration 993, loss = 0.53140237\n",
            "Iteration 994, loss = 0.53164654\n",
            "Iteration 995, loss = 0.53119279\n",
            "Iteration 996, loss = 0.53041709\n",
            "Iteration 997, loss = 0.53019030\n",
            "Iteration 998, loss = 0.53086243\n",
            "Iteration 999, loss = 0.52942903\n",
            "Iteration 1000, loss = 0.52845090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100),\n",
              "              learning_rate='adaptive', max_iter=1000, solver='sgd',\n",
              "              verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "plot(classifier.loss_curve_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "9IChOBsGSFwK",
        "outputId": "9452284c-a38e-425c-bc6d-360fd5ad2a13"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd06ed18590>]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Use the trained model to generate text"
      ],
      "metadata": {
        "id": "aSGTsbN4Tj6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_id = lambda id_sequence: classifier.predict([tokeniser.decode(id_sequence).vector])"
      ],
      "metadata": {
        "id": "7M6bhVTl9MKb"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import concatenate, array"
      ],
      "metadata": {
        "id": "1ixyNCTB_bW_"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt:str) -> str:\n",
        "  sequence = tokeniser.encode(prompt).ids\n",
        "  for _ in range(10):\n",
        "    sequence = concatenate([sequence,predict_next_id(sequence)])\n",
        "  return str(tokeniser.decode(sequence))"
      ],
      "metadata": {
        "id": "cFyM-GiM99H6"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"the jury\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "YRaaGpqs9uAV",
        "outputId": "460066ea-02e8-42fd-cbc7-81494550ab54"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:330: RuntimeWarning: divide by zero encountered in log\n",
            "  log_a = np.log(np.array(a, dtype=dtype))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the jury said it found that county <Unknown> committee for and <Unknown>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Save the trained model"
      ],
      "metadata": {
        "id": "Oatz_hkMTofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "PATH = \"language_model.joblib.pkl\"\n",
        "dump(classifier, PATH, compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FOkQ2L9Sb_I",
        "outputId": "394cb407-91ff-4b74-86be-502c4cb4e4e8"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language_model.joblib.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    }
  ]
}