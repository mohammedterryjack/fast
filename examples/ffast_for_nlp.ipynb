{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ffast in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (0.1.5)\n",
      "Requirement already satisfied: jellyfish in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from ffast) (0.9.0)\n",
      "Requirement already satisfied: scipy in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from ffast) (1.7.3)\n",
      "Requirement already satisfied: Unidecode in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from ffast) (1.3.4)\n",
      "Requirement already satisfied: numpy in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from ffast) (1.22.2)\n",
      "Requirement already satisfied: nltk in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from ffast) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from nltk->ffast) (2022.1.18)\n",
      "Requirement already satisfied: tqdm in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from nltk->ffast) (4.62.3)\n",
      "Requirement already satisfied: click in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from nltk->ffast) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/mohammedterry-jack/testground/lib/python3.9/site-packages (from nltk->ffast) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ffast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffast import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model (Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mohammedterry-\n",
      "[nltk_data]     jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mohammedterry-\n",
      "[nltk_data]     jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokeniser = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[117797, 117730, 117659, 105137]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.decode([117797, 117730, 117659, 105134])\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = this\n",
       "        morphology = this\n",
       "        phonology = 0S\n",
       "        id = 117797\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = is\n",
       "        morphology = is\n",
       "        phonology = IS\n",
       "        id = 117730\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = a\n",
       "        morphology = a\n",
       "        phonology = A\n",
       "        id = 117659\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = test\n",
       "        morphology = test\n",
       "        phonology = TST\n",
       "        id = 105134\n",
       "        tag = Verb\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'be.v.01'}\n",
       "        definition = show a certain characteristic when tested\n",
       "        example = He tested positive for HIV\n",
       "        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound words (e.g. 'big shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 8\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = the\n",
       "        morphology = the\n",
       "        phonology = 0\n",
       "        id = 117788\n",
       "        tag = <StopWord>\n",
       "        similar = set()\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = set()\n",
       "        definition = None\n",
       "        example = None\n",
       "        \n",
       "\n",
       "        text = big shot\n",
       "        morphology = big shot\n",
       "        phonology = BK XT\n",
       "        id = 11031\n",
       "        tag = Noun\n",
       "        similar = {'big_fish', 'big_gun', 'head_honcho', 'big_cheese', 'big_enchilada', 'big_wheel', 'big_shot', 'big_deal'}\n",
       "        opposite = set()\n",
       "        related = {'knocker', 'colloquialism', 'supremo'}\n",
       "        semantics = {'living_thing.n.01', 'entity.n.01', 'adult.n.01', 'physical_entity.n.01', 'organism.n.01', 'important_person.n.01', 'object.n.01', 'causal_agent.n.01', 'person.n.01', 'whole.n.02'}\n",
       "        definition = an important influential person\n",
       "        example = he thinks he's a big shot; she's a big deal in local politics; the Qaeda commander is a very big fish\n",
       "        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"the big shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tokens (i.e. for Entity Extraction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now redfox'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"i will fly to the nasa space station now redfox\")\n",
    "str(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'will', 'fly', 'to', 'the', 'nasa', 'space station', 'now', 'redfox']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str,outputs.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will fly to the nasa space station now'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_unknowns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station redfox'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasa space station'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.nouns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.verbs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly nasa space station redfox'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (e.g. \"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 38980\n",
       "        tag = Noun\n",
       "        similar = {'fasting'}\n",
       "        opposite = set()\n",
       "        related = {'diet', 'Ramadan', 'hunger_strike', 'dieting'}\n",
       "        semantics = {'act.n.02', 'entity.n.01', 'event.n.01', 'control.n.05', 'abstraction.n.06', 'abstinence.n.02', 'psychological_feature.n.01', 'self-denial.n.02', 'activity.n.01'}\n",
       "        definition = abstaining from food\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"fast from food in Ramadan\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fast\n",
       "        morphology = fast\n",
       "        phonology = FST\n",
       "        id = 41354\n",
       "        tag = Adjective\n",
       "        similar = {'quick', 'flying'}\n",
       "        opposite = set()\n",
       "        related = {'hurried'}\n",
       "        semantics = set()\n",
       "        definition = hurried and brief\n",
       "        example = paid a flying visit; took a flying glance at the book; a quick inspection; a fast visit\n",
       "        "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode(\"i'm going too fast\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a big_fish \n",
      "he is a big_gun \n",
      "he is a head_honcho \n",
      "he is a big_cheese \n",
      "he is a big_enchilada \n",
      "he is a big_wheel \n",
      "he is a big_shot \n",
      "he is a big_deal \n",
      "he is a big shot\n"
     ]
    }
   ],
   "source": [
    "for variant in tokeniser.encode(\"he is a big shot\").paraphrase():\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Token Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = tuna\n",
       "        morphology = tuna\n",
       "        phonology = TN\n",
       "        id = 108939\n",
       "        tag = Noun\n",
       "        similar = {'Anguilla_sucklandii'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'soft-finned_fish.n.01', 'bony_fish.n.01', 'living_thing.n.01', 'aquatic_vertebrate.n.01', 'entity.n.01', 'teleost_fish.n.01', 'whole.n.02', 'physical_entity.n.01', 'vertebrate.n.01', 'animal.n.01', 'fish.n.01', 'organism.n.01', 'object.n.01', 'chordate.n.01', 'eel.n.02'}\n",
       "        definition = New Zealand eel\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna = tokeniser.encode(\"tuna\")[0]\n",
    "tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fish and chips\n",
       "        morphology = fish and chip\n",
       "        phonology = FX ANT XPS\n",
       "        id = 40416\n",
       "        tag = Noun\n",
       "        similar = {'fish_and_chips'}\n",
       "        opposite = set()\n",
       "        related = set()\n",
       "        semantics = {'matter.n.03', 'entity.n.01', 'physical_entity.n.01', 'dish.n.02', 'nutriment.n.01', 'substance.n.07', 'food.n.01'}\n",
       "        definition = fried fish and french-fried potatoes\n",
       "        example = \n",
       "        "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokeniser.encode(\"i ate fish and chips on the weekend\")[:]\n",
    "tuna.most_similar(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Contextual Sentence Embeddings (dot similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178620,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a test\")\n",
    "outputs.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an exam'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = outputs.most_similar([\n",
    "    tokeniser.encode(\"test this is\"),\n",
    "    tokeniser.encode(\"this is an exam\"),\n",
    "    tokeniser.encode(\"bla bla bla food\")\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('physical_entity.n.01') at depth 6\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London has 9,787,426 inhabitants at the 2011 census'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tokeniser.encode(\"How big is London\").most_similar([\n",
    "    tokeniser.encode(\"London has 9,787,426 inhabitants at the 2011 census\"),\n",
    "    tokeniser.encode(\"London is known for its financial district\"),\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively - Use the Poincare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = load(\"poincare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedterry-jack/testground/lib/python3.9/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = this\n",
       "        morphology = this\n",
       "        phonology = 0S\n",
       "        id = 20\n",
       "        \n",
       "\n",
       "        text = is\n",
       "        morphology = is\n",
       "        phonology = IS\n",
       "        id = 6\n",
       "        \n",
       "\n",
       "        text = a\n",
       "        morphology = a\n",
       "        phonology = A\n",
       "        id = 5\n",
       "        \n",
       "\n",
       "        text = asdasDS\n",
       "        morphology = <Unknown>\n",
       "        phonology = ASTSTS\n",
       "        id = 50001\n",
       "        "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokeniser.encode(\"this is a asdasDS\")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(outputs.skip_unknowns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a <Unknown>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(tokeniser.decode([20,6,5,50001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = tuna\n",
       "        morphology = tuna\n",
       "        phonology = TN\n",
       "        id = 18955\n",
       "        "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna = tokeniser.encode(\"tuna\")[0]\n",
    "tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna.semantics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = tokeniser.encode(\"i ate fish and chips on the weekend\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        text = fish\n",
       "        morphology = fish\n",
       "        phonology = FX\n",
       "        id = 1671\n",
       "        "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuna.most_similar(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an exam'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tokeniser.encode(\"this is a test\").most_similar([\n",
    "    tokeniser.encode(\"test this is\"),\n",
    "    tokeniser.encode(\"this is an exam\"),\n",
    "    tokeniser.encode(\"bla bla bla food\")\n",
    "])\n",
    "str(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.vector.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testground",
   "language": "python",
   "name": "testground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
